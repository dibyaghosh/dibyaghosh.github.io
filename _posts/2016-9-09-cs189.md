---
layout: notes
title: Multivariate Random Vectors
description: Extending single variable probability propositions to multiple dimensions
class: cs189
---



A random vector is a vector whose entries $[x_1 \dots x_n]^T$ are all random variables. These can be thought of as extensions and convenient notations for multivariate distributions. 

The mean of a random vector $\Exp[x]$ is denoted by $\mu$, and each element of the mean  corresponds to the expected value of its random variable.

The covariance matrix of $X$, denoted by $$\Sigma = Cov(X) = \Exp[(x-\mu)(x-\mu)^T]$$ encodes important information about the pairwise covariances of the variables. In particular, we have that $\Sigma_{ii} = Var(X_i)$ and that $\Sigma_{ij} = Cov(X_i,X_j)$\\
Here are some hard-and-fast rules about multivariate probability in case you forgot:
	$$
	\begin{align*}
	P(x) &= \text{Probability density}\\
	P(x =[a_1 \dots a_n]^T) &= \frac{\partial^n}{\partial x_1 \dots \partial x_n} P(x_1 \leq a_1 \dots x_n \leq a_n)\\
	1 &= \int p(x) dx_1 \dots dx_n\\
	P(x_i) &= \int p(x) dx_1\dots dx_{i-1}dx_{i+1}\dots dx_n
	\end{align*}
	$$

**Theorem:** $\Sigma$ is positive semi-definite

*Proof:* Consider an arbitrary vector $v$. We have that $v^T\Sigma v = v^T \Exp[(x-\mu)(x-mu)^T]v = \Exp[v^T(x-\mu)(x-\mu)^Tv] = \Exp[\|(x-\mu)^Tv\|_2^2]$, and thus $v^T\Sigma v \geq 0$

**Theorem:** Let $Z = Ax + b$ where $A$ is a pre-known matrix and $b$ a preknown vector. We have that $\Exp[Z] = A\Exp[x] + b$ and that $\Sigma_Z = A \Sigma_X A^T$. 

*Proof*: The first part follows immediately from linearity of expectation. We shall show that $\Sigma_Z = A \Sigma X A^T$. 

$$
\begin{align*}
\Sigma_Z &= \Exp[(Z - \Exp[Z])(Z - \Exp[Z])^T]\\
&= \Exp[(Ax + b - A\Exp[x] - b)(Ax + b - A\Exp[x] - b)^T]\\
 &=\Exp[(Ax - A\Exp[x])(Ax - A\Exp[x])^T]\\
 &= \Exp[A(x - \Exp[x])(x-\Exp[x])^TA^T]\\
&= A\Sigma_XA^T
\end{align*}
$$

**Theorem:** Let $Z = Ax + By + c$. We have that $$\Exp[Z] = A\Exp[X] + B\Exp[Y] + c$$ $$\Sigma_Z = A\Sigma_XA^T + B\Sigma_YB^T + A\Sigma_{XY}B^T + B\Sigma_{YX}A^T$$
where $\Sigma_{XY} = \Exp[(X-\Exp[X])(Y-\Exp[Y])^T]$
{% marginnote 'ppdon' "Proof is left for brevity. See here for more \textit{Insert Lin kHere}" %}

<h2> Multivariate Gaussians </h2>

We extend the gaussian to multiple variables, making it uniquely determined by it's mean and variance.
{% marginnote 'nderiv' "To see a good derivation, see " %}

$$P(X) = \mathcal{N}(\mu,\Sigma ) = \frac{1}{|2\pi\Sigma|^{\frac{1}{2}}} \exp\left(-\frac{1}{2} (x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$

Just like their one-dimensional counterparts, multivariate gaussians are closed under linear transformations. 