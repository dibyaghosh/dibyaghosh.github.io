---
layout: notes
title: Machine Learning Abstractions
description: Developing general tools for dealing with various machine learning models
class: cs189
---

We often write our input *data* as $x_1 \dots x_n \in \RR^d$. Our standard for writing *labels* $y_1 \dots y_n$ differs dependent on the type of question.

- In *Unsupervised Learning*: the labels are not given
- In *Classification*, the labels in $\{0,1\}$
-  In *Multi-class classification*, the labels are in $\{0,1 \dots k-1\}$
- In *Regression*, the labels are in $\RR$
\end{enumerate}

We define a model as a function $F: X \to Y$. It's corresponding \textit{optimization problem} can be given by 
$$ \min_{model} \text{Risk + }\lambda * \textbf{Model Capacity}$$ where $\lambda$ is the regularization parameter. The algorithms to optimize include 

- Gradient Descent
- Stochastic Gradient Descent
- Singular Value Decomposition {% sidenote 'svd' ' SVD involves much more intense optimization of matrices' %}

<h2> Risk </h2>

Oftentimes when optimizing a model,we aren't attempting to minimize the largest loss, but rather attempting the minimize the *average* loss. Given a loss function $loss(prediction,y)$, we can construct the risk function as follows 
$$ R[w] = \Exp[loss] = \int loss(pred,y)p(x,y) ~~dx dy$$

However, the issue is that we don't know $P(x,y)$, the distribution of the data. The only value that we do know is the sample risk

$$ R_{s}[w] = \frac{1}{n}\sum_{1}^n loss(pred_i,y_i)$$

We assume that when $n$ is large, that $R_{s}[w] \approx R[w]$.

<h2> Empirical Risk Minimization </h2>

*Empirical Risk Minimization* is the process of minimizing the sample risk $R_s[w]$, in the hopes that this translates to a low risk model. 

We define the **Training Error** $ err_t = \frac{ \text{Number Incorrect Examples}}{\text{ Number Training Examples}}$. 
 
Further, we define the **Prediction Error** $err_p$ to be the error rate on unseen data. Notice that if we assume an infinite amount of possible data (from the model), then $err_p = R[w]$

Finally, we define the **Generalization Error** to be the difference in the prediction error and the training error.  $err_g = err_p - err_t$. 

With this notation, we can see that 

$$	\text{Prediction Error = Generalization Error  + Training Error}$$
	$$ R[w] = (R[w] - R_s[w]) + R_s[w]$$
$$ \text{Prediction Error = Generalization Error }$$

<h2> Losses and Regularizations </h2>

Common Losses for classification include

- Hinge Loss is $L = \max (1 - yw^Tx, 0)$
- Least Squares Loss is $L = (1 - yw^Tx)^2 = (y - w^Tx)^2$
- Logistic Regression Loss is $L = -yx + \log( exp(-w^Tx) + exp(w^Tx))$

For *regression*, the loss is often $(y - w^Tx)^2$

Common Regularizations are the $l_0, l_1, l_2$ metrics.

<h2> Maximum Likelihood Estimation </h2>

Consider a model of the data which believes that there is some generative model of a given type with parameters $\Theta$ from which we sample $(x,y)$ pairs.

Our goal is to maximize the probability that the model outputted the values $\{(y_i,x_i)\}_{i \in N}$. That is, we are trying to maximize
$$\max_\Theta P[\{(y_i,x_i)\} | \Theta]$$.

In order to simplify calculations, we assume that the samples are independent, giving us

$$ \max_\Theta \prod_{i=1}^n P(x_i,y_i | \Theta)$$

In order to further simplify (and since probabilities aren't always concave), we often take the log-likelihood (since most functions are log-convex). Thus, this is equivalent to maximizing
$$\max_\Theta \sum \log P(x_i,y_i | \Theta)$$

<h2> Nonlinear Decision Boundaries </h2>

The easiest way to create nonlinear decision boundaries is to create more featues, thus lifting the data into a higher dimension.

For example, we can add a bias term by considering the new vector $[ x~~1]^T$ given some vector $x$. We can add features (for example, adding all pairwise interactions between variables). However, this can be very costly, since transforming into a new feature space takes on the order of $O(nd')$. 