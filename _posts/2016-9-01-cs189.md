---
layout: notes
title: Optimization
description: Exploring generalized optimization techniques for loss functions, including GD, SGD, and Newton's Method. 
class: cs189
---

Recall from the last <a href="#">note</a> that the loss function that we attempt to minimize for Support Vector Machines is given by 
$$f(w) = \sum_{i=1}^n (1 - y_iw^Tx_i)_+ + \|w\|_2^2$$

What does it mean to minimize a function? 

We call a point $w'$ a *minimizer* (global minimum) if $\forall w~~ f(w^*) \leq f(w)$. 

We call a point $w'$ a *local minimizer*(local minimum) if $\exists R> 0$ $\forall w \in B(w',R)~~f(w') \leq f(w)$

<h2> Convexity </h2>
{% marginnote "convexpic" "<img src='http://www.me.utexas.edu/~jensen/ORMM/models/unit/nonlinear/subunits/terminology/graphics/convex1.gif'> Here's a picture of a convex function and a secant line "%}


We call a function $f$ *convex* if $\forall w_1, w_2 \in \RR^d ~~ t \in [0,1]$ 

$$f(tw_1 + (1-t)w_2) \leq tf(w_1) + (1-t)f(w_2)$$

This is equivalent to saying that, given any two points, the function falls underneath the line that connects the two end points.

Here are some ways of checking whether or not a function is convex

1. Constant functions $f(x)=c$ are convex 
2. Powers of $x$: ($f(x) = x^r$) are convex on the interval $[0,\infty]$ when $r \geq 1$
3. If $f(x)$ is convex, then $f(\mathbf{w^Tx} + \beta)$ is also convex.
4. $-\log(x)$ is convex (Negative Log Likelihood works because of this)
5. If $f(x)$ is concave, then $-f(x)$ is convex.
6. If $f(x)$ and $g(x)$ are convex, then $h(x) = \max \{f(x),g(x)\}$ is convex.

<h2> Gradient Descent </h2>

Gradient descent is a procedure for attempting to find the minimum value of some unbounded function which has a defined gradient at that point. The update equation at each step goes as following:

$$ w_{k+1} = w_{k} - \alpha \nabla f(w_{k})$$ 
where $\alpha$ is the learning parameter. We claim that $-\nabla f(W_{k})$ is the direction of greatest descent. 

*Proof:* Recall that an approximation for $f(w) $ is $f(w_k) + \nabla (w - w_k) + o((w-w_k)^T(w-w_k)) $. 

Considers steps of size $1$, so that $w' = w + \delta$, where $\|\delta\|_2^2 = 1$.
We are attempting to maximize $f(w) -f(w')$ (This is the drop in the function).

$$
\begin{align*}
\arg \max_{\delta} f(w) - f(w') &= \arg \max_{\delta} f(w) - f(w + \delta)\\
 &= \arg \max_{\delta} f(w) - \left(f(w) + \nabla^T((w + \delta) - w)  + o(\|w + \delta - w\|_2^2)\right)\\ 
&= \arg \max_{\delta} -\nabla^Tw - \nabla^T\delta - o(1)\\
\end{align*}
$$
Notice that $\nabla^Tw$ is a constant factor, and we further disregard higher-order terms (the $o(1)$)
$$
\begin{align*}
&= \arg \max_{\delta}  -\nabla^T\delta \\
\end{align*}
$$
Recall from the first homework, that $\arg \max_{\|x\| = 1}x^Tz = \frac{z}{\|z\|_1}$, and thus the above expression is maximized when
$$
\begin{align*}
\delta &= \frac{-\nabla}{\|-\nabla\|_1}\\
\delta \propto -\nabla
\end{align*} 
$$
Thus, the direction of the steepest descent is the opposite of the gradient.


If $w$ is a local minimum, then we have that $\nabla f(w_0) = 0$. The proof follows from the previous part, since if $w$ is a local minimum, there is no descent direction, and thus $\nabla f$ must be 0.However, the converse is not necessarily true.

**Theorem:** If *f* is convex, then $w_*$ is a global minimum if and only if $\nabla f = 0$. 

*Proof:* It is clear from the previous part that if $w'$ is a global minimum, then $\nabla f = 0$. We prove then that $\nabla w' = 0$ implies that $w'$ is a global minimum.

Recall that since $f$ is convex, we have that $\forall w_1,w_2 ~~ \forall t \in [0,1] ~~ f(t_1w_1 + (1-t_1)w_2) \leq t_1f(w_1) + (1-t_1)f(w_2)$. Consider a point $w^*$ such that $\nabla w' = 0$. Further, consider an arbitrary other point $w$
$$
 \begin{align*}
  f(tw + (1-t)w') &\leq tf(w) + (1-t)f(w')\\
  f(w^* + t(w - w')) - (1-t)f(w')&\leq tf(w)\\
  \frac{f(w^* + t(w - w')) - (1-t)f(w')}{t} &\leq f(w)\\
  f(w^*) + \frac{f(w' + t(w - w')) - f(w)}{t} &\leq f(w)\\
  \end{align*}
$$
Let's consider what happens as $t \to 0$
$$
\begin{align*}
  f(w') + \lim_{t \to 0} \frac{f(w' + t(w - w')) - f(w)}{t}  &\leq f(w)\\
  \end{align*}
$$

Notice that this is just a directional derivative
$$
\begin{align*}
  f(w') + \nabla f \dot (w - w') &\leq f(w)\\
  f(w') &\leq f(w)
 \end{align*}
$$

<h2> Psuedocode </h2>
<pre> <code class="python">
def gradientDescent(loss,w,learningRate,steps):
	 for i in range(steps):
		 w = w - learningRate * (loss.gradient(w))
</code></pre>

 Here's an example of computing the gradient of, and the step equation of the SVM soft-margin classifier. 
 
 First, consider the gradient of $f(x) = max(1 - y_i(w^Tx_i),0)$. This is
 
 $$ \nabla f = \begin{cases}
 0 & y_i(w^Tx_i) > 1\\
 \text{Undefined} & y_i(w^Tx_i) = 1\\
 -y_ix_i & y_i(w^Tx_i) < 1
 \end{cases})$$
 
Since the gradient is undefined when $y_i(w^Tx_i) = 1$, we let it be $0$. (Do you notice any connections with the perceptron?). 

Thus, the overall gradient is $c\sum \nabla(f(x_i))  +2w$

<h2>Stochastic Gradient Descent</h2>

Gradient descent is a powerful algorithm, but it can potentially be very slow. In particular, a single iteration of gradient descent takes time $O(n)$ where $n$ is the number of training points that we have. In the case that $n$ is very large, these iterations are quite costly. Thus, we introduce a new idea.

Instead of computing the gradient on the entire training set, we compute the gradient of the loss on a random data point picked at each iteration, and travel down, this way.

Assuming that we select a random point $i: (x_i,y_i)$, notice that the gradient of loss is a noisy version of the true gradient. Thus,
$$\mathbb{E}(\nabla loss_i(w)) = \nabla loss(w)$$

Notice that just because $\Exp(\nabla loss_i(w^*)) = 0$, it doesn't mean that $\forall i~~loss_i(w^*) = 0$. Similarly, just because the current loss that we created has a zero gradient, it doesn't mean that we have a local minimum.

<h3> Why Randomness? </h3>

Why do we use random points, instead of points in the order of the dataset? It could be that the data is arranged in a bad (perhaps even adverserial way) such that the time till convergence is significantly increased. If we select random particles, the probability that we get such a bad sequence is reduced to a low number.

<h3> Region Of Confusion </h3>

Let us assume that we can draw the individual loss functions for each data point. They (looking at it in 2 dimensions for simplicity) form a picture like this \\~\\
\marginnote{As per Dimitri Bertsekas}
\includegraphics*[]{regionofconfusion}
What this shows us is that when the weight is at the end, the stochastic gradients tend to align and agree, and so we have fast and uniform movement in the "farout" regions. However, once in the center, the individual data points begin to disagree, causing the region of confusion. This eventually leads to bouncing around, causing a slowdown, and prevention of SGD. We can prevent this using the tips below.

<h2> Tips for SGD</h2>

<h3> Epochs </h3>

Instead of choosing an element at random every time, we can shuffle the data once, and run through this shuffled order (which is essentially random without replacement), and repeat once finished. This speeds up the procedure inside the epoch.
	
<h3>Decaying Learning Rate</h3>
	
As a general rule of thumb, you should pick step sizes as large as possible without diverging. You can further anneal the learning rate every epoch($N = N/2$ every epoch), or use a form of exponential decay ($N(t) = (1- \lambda)N(t-1) \approx N_0 e^{-\lambda t}$). 
	
<h3>Momentum</h3>

Essentially emulating a ball rolling down the gradient hill, we pretend as though the ball has some momentum (and thus velocity), and this allows us to guide towards the optimum faster. If the previous direction was good, then it also continues down the direction. (By default, start with momentum parameter $\beta = 0.9$)
	
$$ v_{i} = \beta v_{i-1} - \alpha \nabla(g)$$
$$ w_{i} = w_{i-1} + v_i$$
	

<h2>Newton's Method</h2>

*This section is from Lecture 12*


Newton's method is another way of finding optima of our loss function, which requires many less iterations in exchange for expensive computations. 

How the method works is by approximating a quadratic polynomial about the point you're currently at, and jump to the minimum of the quadratic (where the derivative of the quadratic is $0$). 

Recall the second order taylor series of $f$ about some point $w$:{% sidenote 'Matrix Derivatives' "The matrix derivatives that we use here are 
$\\frac{\\partial f(x)^Tg(x)}{x} = \\frac{\\partial f(x)}{x}g(x) + \\frac{\\partial g(x)}{x}f(x)$
$ \\frac{\\partial g(x)^TAf(x)}{x} = \\frac{\\partial g}{x}Af(x) + \\frac{\\partial f}{x}A^Tg(x)$
"
%}
 
$$
\begin{align*}
f(w') &= f(w) + \nabla^T(w'-w) + \frac{1}{2}(w'-w)^T\nabla^2 (w'-w)\\
\end{align*}
$$
We'd like to find the optimum over all $w'$  and so we take the derivative of this expression w.r.t. $w$ $$
\begin{align*}
\frac{\partial f(w')}{\partial w'} &= -\nabla + \frac{1}{2}\left((-I)(\nabla^2(w'-w)) + (-I)(\nabla^{2T}(w'-w)\right)\\
\text{Recall that the Hessian is symmetric}&\\
\frac{\partial f(w')}{\partial w'} &= -\nabla + \frac{1}{2}\left(-(\nabla^2(w'-w))-(\nabla^{2}(w'-w)\right)\\
\frac{\partial f(w')}{\partial w'} &= -\nabla -(\nabla^2(w'-w))\\
\end{align*}
$$
Setting this to $0$, and solving for $w'$
$$
\begin{align*}
0 &= \nabla + \nabla^2(w'-w)\\
\nabla^2w - \nabla &= \nabla^2w'\\
(\nabla^2)^{-1}(\nabla^2w - \nabla) &= w'\\
w' &= w - (\nabla^2)^{-1} \nabla
\end{align*}
$$

Although this is a nice closed-form solution for the next update step, due to numerical instability with solutions of the inverse of $\nabla^2$, we often solve the step right before it.\sidenote{For example, with scipy.linalg.solve} 
$$w' = (\nabla^2)^{-1}(\nabla^2w - \nabla)$$

Newton's method can also be analogized to the version you learned in single-variable calculus, as trying to find the *zeros* of the derivative, in order to find optima.

Newton's method often converges much faster (in the number of iterations), since it exploits more information about the local surface than gradient descent does. In fact, due to the construction of the problem, it will always converge in exactly $1$ step on quadratics, and have superior performance on loss functions that are similar to quadratic.

However, Newton's method comes at a high computational cost. We have to compute the Hessian of the function (which has $d^2$ entries) on each iteration (since our function changes). In practice, Newton's method is not used, simply because of cost of computing the Hessian Matrix. However, alternative methods such as BFGS, which approximate portions of the Hessian, often work *almost as well*, and are often used in the field. 