---
layout: notes
title: Generative Models
description: Gaussian Discriminant Analysis and it's many variants
class: cs189
---

In contrast with the *empirical risk minimization* that we performed in the previous chapter, we will now be looking at generative models.

A *generative model* is often set up like this. We assume some probability distribution for the $x$ data-points given each class, and attempt to fit the MLE estimate of $P(x \| y = c)$. Using the data given, we also estimage $P(y=c)$. When required to predict, we choose the $y$ that maximizes $P(Y \| X) \propto P(Y)P(X \| Y)$.

<h2> Gaussian Discriminant Analysis </h2>

This form of generative models assumes that the distribution of X given a class is Gaussian. Given classes $y = y_1,y_2\dots y_c$.

The fits that we have (using MLE), for each class is  (letting $\|c\| = \|\text{\# Examples with } y_i = c\|$)
$$\hat{\mu}_{c} = \frac{1}{|c|}\sum_{j: y_j= c} x_j$$
$$\hat{\Lambda}_{c} = \frac{1}{|c|}\sum_{j: y_j= c} (x_j-\hat{\mu}_c)(x_j-\hat{\mu}_c)^T$$
$$\hat{P}(y=c) = \frac{|c| }{n}$$

Our decision rule is then
$$ 
\begin{align*}
\hat{y} &= \arg \max_{c} p(x \| y = c)p(y=c)\\
\end{align*}
$$

Notice that this is equivalent to minimizing the negative log likelihood, and thus picking the *c* that minimizes
$$\begin{align*}
f(c) &= -\log p(x \| y = c) - \log p(y=c)\\
\text{Substituting in, we get }\\
&= -\log (|2\pi\Lambda\|^{-1/2} exp(-\frac{1}{2}(x-\mu)^T\Lambda^{-1}(x-\mu)) - \log \frac{|c|}{n}\\
&= \frac{1}{2} (x-\mu)^T\Lambda^{-1}(x-\mu) + \frac{1}{2} \log |2\pi\Lambda| - \log \frac{|c|}{n}
\end{align*}
$$

**Quadratic Discriminant Analysis** (QDA) is the simple process of selecting $\hat{y}$ by attempting to minimize the negative log likelihood presented above. In the two-class case, we can write our prediction function as 
<pre><code>
if f(class1) - f(class2) > 0:
	return class1
return class2
</code></pre>

<h2> Linear Discriminant Analysis </h2>

As compared to *Quadratic Discriminant Analysis*, in Linear Discriminant Analysis, we assume that the covariance matrix is the same across all the classes.
That is, we presume

$$ \hat{\Lambda_{overall}} = \frac{1}{n} \sum_{i=1}^k (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T$$

where our $\mu_k$ are similarly defined from the previous problem. We shall show that this distills into a linear classifier, in the two sample case. 

Notice that our decision boundary occurs when (letting $y \in \{c_1,c_2\}$) 
$$f(c_1) - f(c_2) = 0$$

Plugging in with our sample means and variances, we have (from the previous part)
$$
\begin{align*}
 \frac{1}{2} (x-\mu_{c1})^T\Lambda^{-1}(x-\mu_{c1}) + \frac{1}{2} \log |2\pi\Lambda| - \log \frac{|c1|}{n} ~~~~~~~& \\- ( \frac{1}{2} (x-\mu_{c2})^T\Lambda^{-1}(x-\mu_{c2}) + \frac{1}{2} \log |2\pi\Lambda| - \log \frac{|c2|}{n}) &= 0\\
 (x-\mu_{c1})^T\Lambda^{-1}(x-\mu_{c1}) - (x-\mu_{c2})^T\Lambda^{-1}(x-\mu_{c2}) - 2\log \frac{|c1|}{n} + 2\log \frac{|c2|}{n} &= 0\\
 -\mu_{c1}\Lambda^{-1}x - x^T\Lambda^{-1}\mu_{c1} + \mu_{c1}^T\Lambda^{-1}\mu_{c1} + \mu_{c1}\Lambda^{-1}x + x^T\Lambda^{-1}\mu_{c1} - \mu_{c1}^T\Lambda^{-1}\mu_{c1} - 2\log \frac{|c1|}{n} + 2\log \frac{|c2|}{n} &= 0\\
\end{align*}
$$

That looks really ugly, but the point is it is a linear classifier (wrt x).

<h2> LDA vs QDA </h2>

It is generally believed that QDA often overfits compared to LDA. In particular, simply noticing that since QDA attempts to fit the covariance matrix for every single class, the number of parameters/degrees of freedom is $\Theta(cd^2)$ whereas for LDA, we only fit one matrix which is $\Theta(d^2)$.