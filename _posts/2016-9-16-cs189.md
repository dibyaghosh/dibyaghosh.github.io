---
layout: notes
title: Bias vs Variance
description: Examining the Bias-Variance tradeoff when minimizing MSE through the James-Stein Estimator
class: cs189
---

<h2> The James-Stein Estimator </h2>

Let's consider a model where $X \sim \mathcal{N}(\mu,\sigma^2 I_d)$, and where we only have one sample, denoted by $x_1$. What are possible estimators of $\mu$?

<h3> MLE Estimate </h3>

The MLE estimate for $\mu$ given only $x_1$ would be $x_1$. {% sidenote "exp0" "This is pretty evident (but if you need evidence, you can see Homework 3- Question 1, or rederive the MLE for the above gaussian)." %}


We have that $\hat{\mu} = x_1$, and we further know that $\Exp[\hat{\mu}] = \Exp[x_1] = \mu$, and so the estimator is *unbiased*.}. Consider the mean squared error of this estimator given by 
$$
\begin{align*}
MSE(\hat{\mu}) &= \Exp[|\hat{\mu}-\mu|_2^2]\\
&= \Exp[|x-\mu|_2^2]\\
&= \Exp[(x-\mu)^T(x-\mu)]\\
&= \Exp[Tr((x-\mu)^T(x-\mu))] & \text{Since this is a scalar}\\
&=  \Exp[Tr((x-\mu)(x-\mu)^T)] & \text{By the cyclic property of traces}\\ 
&= Tr(\Exp[(x-\mu)(x-\mu)^T]) & \text{By linearity of expectation}\\
&= Tr(\Lambda)\\
&= d\sigma^2
\end{align*}
$$

<h3> Devolving into Bias </h3>

Consider a new estimator, where we let $\hat{\mu} = \alpha x_1$, which is clearly biased. {% marginnote "exp1" "We have that $\Exp[\hat{\mu}] = \alpha \Exp[x_1] = \alpha \mu \neq \mu$ and thus is biased" %} What's the mean square error of this estimator?

$$
\begin{align*}
MSE(\hat{\mu}) &= \Exp[|\hat{\mu}-\mu|_2^2]\\
&= \Exp[(\alpha x- \alpha\mu + \alpha\mu - \mu)^T(\alpha x- \alpha\mu + \alpha\mu - \mu)]\\
&= \Exp[(\alpha x - \alpha\mu)^T(\alpha x - \alpha\mu)] - 2\Exp[(\alpha x - \alpha\mu)^T(\alpha \mu - \mu)] + \Exp[(\alpha \mu - \mu)^T(\alpha\mu -\mu)]\\
&= \alpha^2\Exp[(x-\mu)^T(x-\mu)] - 2(\alpha^2 -\alpha)\Exp[(x - \mu)^T\mu] + (\alpha-1)^2 \|\mu\|^2\\
&= \alpha^2\Exp[(x-\mu)^T(x-\mu)] + (\alpha-1)^2 \|\mu\|^2\\
&= \alpha^2\sigma^2d + (\alpha-1)^2 \|\mu\|^2
\end{align*}
$$

Notice that this mean squared error can potentially be smaller than that of our MLE estimator, and thus we can reduce the error by choosing an appropriate $\alpha$.

<h3> And Voila! The James Stein Estimator </h3>

Assuming that we know, a priori, the value of $\sigma^2$, we can construct a new estimator where 
$$\alpha = 1 - \frac{(d-2)\sigma^2}{\|x\|^2}$$.

Although the proof is rather ugly, we can show that this estimator will always have a better mean squared error compared to our MLE estimate when we are looking in dimensions $d > 2$

<h2> Bias vs Variance </h2>

We define the **bias** of an estimator $\hat{\mu}$ to be
$$ bias(\hat{\mu}) = \|\Exp[\hat{\mu}-\mu]\|_2^2$$

We define the **variance** of an estimator $\hat{\mu}$ to be
$$var(\hat{\mu}) = \Exp[\|\hat{\mu}-\Exp[\hat{\mu}]\|_2^2]$$

Let's assume that our loss function is of the form $\|\hat{\mu} - \mu\|_2^2$. Consider where we have $y = f(x)$, and we try to fit a new predictor $\hat{f}$ to model $f$. What's the loss? First, it's important to notice what we are integrating over: we are integrating over the underlying model parameters $\Theta$, which is equivalent to taking the expectation over $X^T$ the training data we recieve, and $x$ the testing data point.
$$
\begin{align*}
Risk(\hat{y}) &= \Exp_{x,X^T}[Loss(\hat{f},f)]\\
&=  \Exp_{x,X^T}[\|\hat{f}(x)-f(x)\|^2]\\
&= \Exp_{x,X^T}[\|\hat{f}(x)-f(x)\|^2]\\
&=\Exp_{x,X^T}[\|\hat{f}(x)-\Exp_{X^T|x}[\hat{f}(x)]+\Exp_{X^T|x}[\hat{f}(x)]-f(x)\|^2]\\
&= \underset{\text{The Variance}} {\Exp_{x,X^T}[\|\hat{f}(x)-\Exp_{X^T|x}[\hat{f}(x)]\|^2]} + 2\Exp_{x,X^T}[(\hat{f}(x)-\Exp_{x,X^T}[\hat{f}(x)]) \cdot (\Exp_x[\hat{f}(x)]-f(x))] + \underset{\text{The Bias}} {\Exp[\|\Exp_{X^T|x}[\hat{f}(x)]-f(x)\|_2^2]} \\
&= Var(\hat{f}) + 2\Exp_{x,X^T}[(\hat{f}(x)-\Exp_x[\hat{f}(x)]) \cdot (\Exp_x[\hat{f}(x)]-f(x))] + Bias(\hat{f})\\
\end{align*}
$$
We split the expectation into an iterated expectation
$$
\begin{align*}
&= Var(\hat{f}) + 2\Exp_{x}\left[\Exp_{X^T | x}\left[(\hat{f}(x)-\Exp_{X^T|x}[\hat{f}(x)]) \cdot (\Exp_{X^T|x}[\hat{f}(x)]-f(x))\right]\right] + Bias(\hat{f})\\
\end{align*}
$$
Notice that the right side of the dot product is constant when taking the expectation w.r.t $X^T$(since $f(x)$ isn't dependent on $X^T$)
$$
\begin{align*}
&= Var(\hat{f}) + 2\Exp_{x}\left[\Exp_{X^T | x}\left[(\hat{f}(x)-\Exp_{X^T|x}[\hat{f}(x)])\right] \cdot (\Exp_{X^T|x}[\hat{f}(x)]-f(x))\right] + Bias(\hat{f})\\
\text{However, this center expectation is just $0$ (it's the deviation)}\\
&= Var(\hat{f}) + 2\Exp_{x}\left[\vec{0} \cdot (\Exp_{X^T|x}[\hat{f}(x)]-f(x))\right] + Bias(\hat{f})\\
&= Var(\hat{f}) + Bias(\hat{f})\\
\end{align*}
$$
For another example, let's say that $y = f(x) + \epsilon$ where $\epsilon$ is randomly distributed around $0$, and $f$ is unknown. How does does our loss function decompose?

$$
\begin{align*}
\Exp[\|\hat{f}(x)  - y\|_2^2] &= \Exp[\|\hat{f}(x) -f(x) - \epsilon\|_2^2]\\
 &= \Exp[\|\hat{f}(x) -f(x)\|^2 + 2(\hat{f}(x)- f(x) \cdot \epsilon) + \| \epsilon\|_2^2]\\
 &= (Bias(\hat{f}) + Var(\hat{f})) + \Exp[2(\hat{f}(x)- f(x) \cdot \epsilon)] + \Exp[\|\epsilon\|^2]\\
 \text{However since this noise is independent}\\
  &= (Bias(\hat{f}) + Var(\hat{f})) + \Exp[\epsilon^T\epsilon)]\\
  &= (Bias(\hat{f}) + Var(\hat{f})) + \Exp[tr(\epsilon^T\epsilon)]\\
  &= (Bias(\hat{f}) + Var(\hat{f})) + \Exp[tr(\epsilon\epsilon^T)]\\
  &= (Bias(\hat{f}) + Var(\hat{f})) + tr(\Exp[\epsilon\epsilon^T])\\
&= (Bias(\hat{f}) + Var(\hat{f})) + tr(\Lambda_\sigma)\\
  &= Bias(\hat{f}) + Var(\hat{f}) + \text{Irreducible Noise}\\
\end{align*}
$$