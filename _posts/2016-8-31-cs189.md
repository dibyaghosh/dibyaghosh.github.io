---
layout: notes
title: Linear Classifiers
description: Studying the mathematical background behind linear classifiers, perceptrons, and SVMs
class: cs189
---


What does machine learning aim to do?

Given *data* $x_1 \dots x_n$ and *properties* $y_1 \dots y_n$, we attempt to predict $y(x)$ for some $x$ that is not in the sample *data*.

Abstractly, we build functions of the type
<pre> <code class="python">
def predict(x):
	y = miracle(x)
	return y
</code>
</pre>
In this class we make the following assumption

$x_i$ live in $\mathbb{R}^d$: we can write each data point $x_i$ as a feature vector, where each entry is a feature. When we consider the data matrix $X$ (with dimensions $n \times d$), we let $X_{ij}$ be the $j$-th feature of the $i$-th example.


<h2> Linear Classifiers </h2>

We consider the following classifier for binary classification (where $Y \in \{0,1\}$). 



We essentially define a scoring function defined by 
$$f(x) = w^Tx + \beta$$ and then classifying $y = 1$ if $f(x) \geq 0$ and $y = 0$ if $f(x) < 0$ \marginnote{This is equivalent $y_i = \mathbb{1}(\beta^T x_i \geq \tau)$ for some threshold $\tau = -\beta$}


<pre> <code class="python">
def predict(x):
	v = np.dot(w,x)
	if v > threshold:
		return True
	return False
</code></pre>

The decision boundary for this linear classifier is a hyperplane which divides the space into two halves. If a new datapoint is one on side of the decision boundary, we classify it as $1$, and otherwise as $0$. The decision boundary is given by 
$$ \{x \in \RR^d: w^Tx + \beta= 0\}$$

<h2> Properties of the Boundary </h2>
**Theorem 1:** {%sidenote 'boundaryproof' "For proof, consider $x,y$ on the plane, so we have that $x-y$ is on the plane. Since $x$ and $y$ are on the plane, we know that $w^Tx + \beta = 0$ and $w^Ty + \beta =0$, and subtracting the equations that $w^T(x-y) = 0$, or that $w \bot (x-y)$" %} We have that $w$ is the normal vector of this hyperplane (that it is orthogonal to all vectors in the plane)


This theorem allows us to notice that the smallest vector that goes from an arbitrary point $x$ to the boundary will be a scalar multiple of $w$. That is, if $\tau$ is the shortest distance to the boundary, then we have that 
$x + \tau \frac{w}{\|w\|} \in \text{Boundary}$

**Theorem:**
{%sidenote 'marginproof' 
"For proof, notice that we are simply solving for $\tau$ when $x = \\vec{0}$. We have that $w^T(\\tau \\frac{w}{\\|w\\|}) + \\beta = 0$. Solving for $\tau$, we have $\\tau \\frac{\\|w\\|_2^2}{\\|w\\|} = -\beta$, or $\\tau = \\frac{-\\beta}{\\|w\\|}$"
%}
The distance from the origin of $\RR^d$ to the decision boundary is given by $- \frac{\beta}{\|w\|}$

**Theorem** The (signed) distance from a vector to the boundary is given by $\frac{f(x)}{\|w\|}$

*Proof:* For an arbitrary vector $x$, the distance to the boundary can be found by solving the original equation for the shortest vector.

{% math %}
\begin{align*}
w^T(x + \tau \frac{w}{\|w\|}) + \beta &= 0\\
\tau \frac{\|w\|^2}{\|w\|}  &= -\beta - w^Tx\\
\tau \|w\| &= -f(x)\\
\tau &= -\frac{f(x)}{\|w\|}
\end{align*}
{% endmath %}

We'd like to use these results to analyze how good a classifier is. For this, we shift notation to let $y_i = \{-1,1\}$, which means that if we multiply $y_if(x_i)$, then if the result is positive, then we have classified correctly; otherwise we have that the classification is incorrect.

We define the *margin* of a classifier as 

$$\text{margin} = \min_{i} \frac{y_i f(x_i)}{\|w\|}$$

Our goal is to now find a classifier that maximizes the margin. To simplify, first assume that the data is linearly separable. This is equivalent to saying that the margin is positive (nonzero and not negative).  Since $\exists w ~~\forall i ~~ (y_i f(x_i)) > 0$, letting $\alpha = \min_{i}\{y_if(x_i)\}$, we know that when $w' = \frac{1}{\alpha} w$, then $\forall i ~~ (y f(x_i)) > 1$. 

We can now use this fact to now try to maximize the margin: $$w' = \max_{w}  \frac{y_if(x_i)}{\|w\|}$$ subject to $y_if(x_i) > 0$. However, we can rewrite this as $$w' = \min_{w} \|w\|$$ subject to $y_if(x_i) > 1$

<h2> Perceptrons </h2>

The previous maximization problem required quadratic programming to solve, and this may not be the best approach. Instead, we introduce perceptrons. In essence, we attempt to train a linear classifier, updating the weights as we go to try to match the data better.

<pre> <code class="python">
for data point (x,y) in data (continue this stream forever):
	if (y*f(x) < 0):
		w = w + y*x
		b = b+ y
</code></pre>

**Theorem:**{%sidenote "perceptronproof" "Proof at <a href='http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf'>http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf</a>"%} If the data is linearly separable, then no matter the order of updates, the perceptron algorithm will terminate with a correct solution. The algorithm will converge in at most $\frac{R^2}{\gamma^2}$, where $R = \max \|x_i\|$ and $\gamma$ is the margin.

<h2> Slack </h2>

Let's assume that the data is not linearly separable; what do we do in order to find a good margin fit. We introduce a concept called *slack*. We relax our constraints to $\forall i ~~ y_iw^Tx_i \geq 1 - \epsilon_i$ where $\epsilon_i > 0$. However, we must reduce incentive to use slack, and we add it to the minimization term; Thus our new minimization becomes

$$ \min_{w} \|w\|_2^2 + c \sum \epsilon_i$$ such that $\forall i ~~ y_iw^Tx_i \geq 1 - \epsilon_i$.

Notice that this is equivalent to minimizing 
$$ \min_{w} c \sum_{i=1}^n (1 - y_iw^Tx_i)_+ + \|w\|_2^2$$
where $(x)_+ = max(0,x)$

Here's what happens when you play around with the parameter $c$:

|          | small C                                         | large C                                   |
|----------|-------------------------------------------------|-------------------------------------------|
| Desire   | maximize margin $\frac{1}{\|w\|}$               | Keep most slack variables small (or zero) |
| Danger   | Underfitting (misclassifies much training data) | Overfitting                               |
| Outliers | less sensitive                                  | very sensitive                            |