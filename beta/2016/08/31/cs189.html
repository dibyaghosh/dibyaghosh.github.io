<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
</head>
<p style="display:none">
$$\def\RR{\mathbb{R}}$$
</p>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<link rel="stylesheet" href="/beta/css/tufte.css">
<article>
 <h1 id="tufte-css">Linear Classifiers</h1>
 <p class="subtitle"> Dibya Ghosh </p>
 <p> August 31, 2016
</p>
<div class="epigraph">
<blockquote>
    <p>Studying the mathematical background behind linear classifiers, perceptrons, and SVMs</p>
    </blockquote>
</div>
      <p>What does machine learning aim to do?</p>

<p>Given <em>data</em> $x_1 \dots x_n$ and <em>properties</em> $y_1 \dots y_n$, we attempt to predict $y(x)$ for some $x$ that is not in the sample <em>data</em>.</p>

<p>Abstractly, we build functions of the type</p>
<pre> <code class="python">
def predict(x):
	y = miracle(x)
	return y
</code>
</pre>
<p>In this class we make the following assumption</p>

<p>$x_i$ live in $\mathbb{R}^d$: we can write each data point $x_i$ as a feature vector, where each entry is a feature. When we consider the data matrix $X$ (with dimensions $n \times d$), we let $X_{ij}$ be the $j$-th feature of the $i$-th example.</p>

<h2> Linear Classifiers </h2>

<p>We consider the following classifier for binary classification (where $Y \in {0,1}$).</p>

<p>We essentially define a scoring function defined by 
<script type="math/tex">f(x) = w^Tx + \beta</script> and then classifying $y = 1$ if $f(x) \geq 0$ and $y = 0$ if $f(x) &lt; 0$ \marginnote{This is equivalent $y_i = \mathbb{1}(\beta^T x_i \geq \tau)$ for some threshold $\tau = -\beta$}</p>

<pre> <code class="python">
def predict(x):
	v = np.dot(w,x)
	if v &gt; threshold:
		return True
	return False
</code></pre>

<p>The decision boundary for this linear classifier is a hyperplane which divides the space into two halves. If a new datapoint is one on side of the decision boundary, we classify it as $1$, and otherwise as $0$. The decision boundary is given by 
<script type="math/tex">\{x \in \RR^d: w^Tx + \beta= 0\}</script></p>

<h2> Properties of the Boundary </h2>
<p><strong>Theorem 1:</strong> <label for="boundaryproof" class="margin-toggle sidenote-number"></label><input type="checkbox" id="boundaryproof" class="margin-toggle" /><span class="sidenote">For proof, consider $x,y$ on the plane, so we have that $x-y$ is on the plane. Since $x$ and $y$ are on the plane, we know that $w^Tx + beta = 0$ and $w^Ty + beta =0$, and subtracting the equations that $w^T(x-y) = 0$, or that $w bot (x-y)$ </span> We have that $w$ is the normal vector of this hyperplane (that it is orthogonal to all vectors in the plane)</p>

<p>This theorem allows us to notice that the smallest vector that goes from an arbitrary point $x$ to the boundary will be a scalar multiple of $w$. That is, if $\tau$ is the shortest distance to the boundary, then we have that 
$x + \tau \frac{w}{|w|} \in \text{Boundary}$</p>

<p><strong>Theorem:</strong>
<label for="marginproof" class="margin-toggle sidenote-number"></label><input type="checkbox" id="marginproof" class="margin-toggle" /><span class="sidenote">For proof, notice that we are simply solving for $tau$ when $x = \vec{0}$. We have that $w^T(\tau \frac{w}{|w|}) + \beta = 0$. Solving for $tau$, we have $\tau \frac{|w|_2^2}{|w|} = -beta$, or $\tau = \frac{-\beta}{|w|}$ </span>
The distance from the origin of $\RR^d$ to the decision boundary is given by $- \frac{\beta}{|w|}$</p>

<p><strong>Theorem</strong> The (signed) distance from a vector to the boundary is given by $\frac{f(x)}{|w|}$</p>

<p><em>Proof:</em> For an arbitrary vector $x$, the distance to the boundary can be found by solving the original equation for the shortest vector.</p>

<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
w^T(x + \tau \frac{w}{\|w\|}) + \beta &= 0\\
\tau \frac{\|w\|^2}{\|w\|}  &= -\beta - w^Tx\\
\tau \|w\| &= -f(x)\\
\tau &= -\frac{f(x)}{\|w\|}
\end{align*}
</script></div>

<p>We’d like to use these results to analyze how good a classifier is. For this, we shift notation to let $y_i = {-1,1}$, which means that if we multiply $y_if(x_i)$, then if the result is positive, then we have classified correctly; otherwise we have that the classification is incorrect.</p>

<p>We define the <em>margin</em> of a classifier as</p>

<script type="math/tex; mode=display">\text{margin} = \min_{i} \frac{y_i f(x_i)}{\|w\|}</script>

<p>Our goal is to now find a classifier that maximizes the margin. To simplify, first assume that the data is linearly separable. This is equivalent to saying that the margin is positive (nonzero and not negative).  Since $\exists w ~~\forall i ~~ (y_i f(x_i)) &gt; 0$, letting $\alpha = \min_{i}{y_if(x_i)}$, we know that when $w’ = \frac{1}{\alpha} w$, then $\forall i ~~ (y f(x_i)) &gt; 1$.</p>

<p>We can now use this fact to now try to maximize the margin: <script type="math/tex">w' = \max_{w}  \frac{y_if(x_i)}{\|w\|}</script> subject to $y_if(x_i) &gt; 0$. However, we can rewrite this as <script type="math/tex">w' = \min_{w} \|w\|</script> subject to $y_if(x_i) &gt; 1$</p>

<h2> Perceptrons </h2>

<p>The previous maximization problem required quadratic programming to solve, and this may not be the best approach. Instead, we introduce perceptrons. In essence, we attempt to train a linear classifier, updating the weights as we go to try to match the data better.</p>

<pre> <code class="python">
for data point (x,y) in data (continue this stream forever):
	if (y*f(x) &lt; 0):
		w = w + y*x
		b = b+ y
</code></pre>

<p><strong>Theorem:</strong><label for="perceptronproof" class="margin-toggle sidenote-number"></label><input type="checkbox" id="perceptronproof" class="margin-toggle" /><span class="sidenote">Proof at <a href="http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf">http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf</a> </span> If the data is linearly separable, then no matter the order of updates, the perceptron algorithm will terminate with a correct solution. The algorithm will converge in at most $\frac{R^2}{\gamma^2}$, where $R = \max |x_i|$ and $\gamma$ is the margin.</p>

<h2> Slack </h2>

<p>Let’s assume that the data is not linearly separable; what do we do in order to find a good margin fit. We introduce a concept called <em>slack</em>. We relax our constraints to $\forall i ~~ y_iw^Tx_i \geq 1 - \epsilon_i$ where $\epsilon_i &gt; 0$. However, we must reduce incentive to use slack, and we add it to the minimization term; Thus our new minimization becomes</p>

<p><script type="math/tex">\min_{w} \|w\|_2^2 + c \sum \epsilon_i</script> such that $\forall i ~~ y_iw^Tx_i \geq 1 - \epsilon_i$.</p>

<p>Notice that this is equivalent to minimizing 
<script type="math/tex">\min_{w} c \sum_{i=1}^n (1 - y_iw^Tx_i)_+ + \|w\|_2^2</script>
where $(x)_+ = max(0,x)$</p>

<p>Here’s what happens when you play around with the parameter $c$:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>small C</th>
      <th>large C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Desire</td>
      <td>maximize margin $\frac{1}{|w|}$</td>
      <td>Keep most slack variables small (or zero)</td>
    </tr>
    <tr>
      <td>Danger</td>
      <td>Underfitting (misclassifies much training data)</td>
      <td>Overfitting</td>
    </tr>
    <tr>
      <td>Outliers</td>
      <td>less sensitive</td>
      <td>very sensitive</td>
    </tr>
  </tbody>
</table>

</article>
</html>