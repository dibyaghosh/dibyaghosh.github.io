<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>&lt;img src='https://dibya.xyz/images/logo.png' style='height:100%'&gt; &lt;/img&gt;</title>
    <description>Hi! My name is &lt;b&gt; Dibya Ghosh &lt;/b&gt; &lt;br /&gt;
I'm a student at UC Berkeley interested in AI and ML.&lt;br /&gt;
 Welcome to my Lair!
</description>
    <link>http://dibya.xyz/beta/</link>
    <atom:link href="http://dibya.xyz/beta/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 18 Oct 2016 22:13:42 -0700</pubDate>
    <lastBuildDate>Tue, 18 Oct 2016 22:13:42 -0700</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Deskewing MNIST V2</title>
        <description>&lt;p&gt;When we write, we often write at angles to the paper, which cause letters and numbers to be skewed. Unfortunately, unlike the human eye, computers cannot easily find similarities between images that are transformations of each other. Thus, the process of &lt;em&gt;deskewing&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Very formally, &lt;strong&gt;deskewing&lt;/strong&gt; is the process of straightening an image that has been scanned or written crookedly — that is an image that is slanting too far in one direction, or one that is misaligned.&lt;/p&gt;

&lt;p&gt;In particular, we model the process of deskewing as an affine transformation. We assume that when the image was created (the skewed version), it is actually some affine  skew transformation on the image $ Image’ = A(Image) + b$ which we do not know. What we do know is that we want the center of mass to be the center of the image, and that we’d like to know the angle at which it was skewed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://geometrypreapteacher.wikispaces.com/file/view/skew.jpg/85225087/skew.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The methodology goes as follows:&lt;/p&gt;

&lt;p&gt;1) Find the center of mass of the image to figure out how much we need to offset the image&lt;br /&gt;
2) Find the covariance matrix of the image pixel intensities (we can use this to approximate the skew of the angle)&lt;/p&gt;

&lt;p&gt;The function &lt;strong&gt;moments&lt;/strong&gt; below, calculates these relevant quantities&lt;/p&gt;

&lt;pre&gt; &lt;code class=&quot;python&quot;&gt;
from scipy.ndimage import interpolation

def moments(image):
    c0,c1 = np.mgrid[:image.shape[0],:image.shape[1]] # A trick in numPy to create a mesh grid
    totalImage = np.sum(image) #sum of pixels
    m0 = np.sum(c0*image)/totalImage #mu_x
    m1 = np.sum(c1*image)/totalImage #mu_y
    m00 = np.sum((c0-m0)**2*image)/totalImage #var(x)
    m11 = np.sum((c1-m1)**2*image)/totalImage #var(y)
    m01 = np.sum((c0-m0)*(c1-m1)*image)/totalImage #covariance(x,y)
    mu_vector = np.array([m0,m1]) # Notice that these are \mu_x, \mu_y respectively
    covariance_matrix = np.array([[m00,m01],[m01,m11]]) # Do you see a similarity between the covariance matrix
    return mu_vector, covariance_matrix
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we’d like to calculate the matrix which will allow us to skew “back” to the original image&lt;/p&gt;

&lt;p&gt;This is given by the following formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}1 &amp; 0 \\ \alpha &amp; 1\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\alpha = \frac{Cov(X,Y)}{Var(X)}$&lt;/p&gt;

&lt;p&gt;Furthermore, we have an offset of $\mu - $ center&lt;/p&gt;

&lt;p&gt;Thus, combining the two,  using the handy &lt;em&gt;interpolation&lt;/em&gt; library from scipy, we complete the method &lt;strong&gt;deskew&lt;/strong&gt; below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def deskew(image):
    c,v = moments(image)
    alpha = v[0,1]/v[0,0]
    affine = np.array([[1,0],[alpha,1]])
    ocenter = np.array(image.shape)/2.0
    offset = c-np.dot(affine,ocenter)
    return interpolation.affine_transform(image,affine,offset=offset)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Using L2 Regularized Regression (Ridge Regression), we have&lt;/p&gt;

&lt;h3 id=&quot;unchanged&quot;&gt;Unchanged&lt;/h3&gt;

&lt;p&gt;Train Accuracy: .8564&lt;br /&gt;
Test Accuracy: .8589&lt;/p&gt;

&lt;h3 id=&quot;deskewed&quot;&gt;Deskewed&lt;/h3&gt;

&lt;p&gt;Train Accuracy: .9103&lt;br /&gt;
Test Accuracy: .9140&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thus by using deskewed features, we automatically boost our accuracy rougly 6%! Crazy!&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Oct 2016 00:00:00 -0700</pubDate>
        <link>http://dibya.xyz/beta/2016/10/18/cs189.html</link>
        <guid isPermaLink="true">http://dibya.xyz/beta/2016/10/18/cs189.html</guid>
        
        
      </item>
    
      <item>
        <title>Maximum Flow</title>
        <description>&lt;h2&gt; Ford-Fulkerson Algorithm &lt;/h2&gt;

&lt;p&gt;The Ford-Fulkerson method is a greedy algorithm that computes the maximum flow in a given network. It is often called a &lt;em&gt;“method”&lt;/em&gt; because the exact approach to selecting augmenting paths isn’t well defined (a definition we’ll see later in the &lt;em&gt;Edmonds-Karp&lt;/em&gt; algorithm).&lt;/p&gt;

&lt;h3&gt; Main Idea &lt;/h3&gt;

&lt;p&gt;Consider a graph in which we’re attempting to find the max-flow from start vertex $s$ to end vertex $t$; As long as there is a path from the source node $t$, we send flow along one of the paths, of size of the smallest capacity on the path (leaving a residual network). Continuing to do so until no path exists, leaves only the maximum flow on the graph.&lt;/p&gt;

&lt;h3&gt; The Code &lt;/h3&gt;

&lt;pre&gt; &lt;code&gt;
def FFFlow(graph,s,t):
	f[edge] = 0 forall edges
	while existsPositivePath(s,t):
		path = getPositivePath(graph,s,t)
		minFlow = min(capacity of path)
		for edge in path:
			f[edge] += minFlow
			graph.capacity[edge] -= minFlow
	return f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we have that &lt;em&gt;getPositivePath&lt;/em&gt; returns a path (if one exists) in the graph, such that the minimum capacity is greater than 0.&lt;/p&gt;

&lt;p&gt;The general gist is that at every iteration, we pick a path with positive flow and append it to our current flow.&lt;/p&gt;

&lt;h3&gt; Runtime Analysis &lt;/h3&gt;

&lt;h3&gt; Proof of Correctness &lt;/h3&gt;

&lt;h2&gt; Edmonds-Karp Algorithm &lt;/h2&gt;

&lt;p&gt;Pick the $s-t$ path in the newtork with the fewest edges (using BFS)&lt;/p&gt;

&lt;p&gt;The Edmonds-Karp algorithm is an extension of the Ford-Fulkerson method for calculating the maximum flow in a network in time $O(VE^2)$&lt;/p&gt;

&lt;h3&gt; Analysis &lt;/h3&gt;

&lt;p&gt;We will show that the number of iterations required for this to converge is &lt;span&gt;​&lt;script type=&quot;math/tex&quot;&gt;\leq EV&lt;/script&gt;&lt;/span&gt;. We do show by showing that the distance (number of edges) of t from s in residual network increases at least every $E$ iterations.&lt;/p&gt;

&lt;p&gt;Before doing so, we consider a Linear Program which models Maximum Flow (this is a very bad linear program in practice, but it provides insight into the process)&lt;/p&gt;

&lt;p&gt;Let &lt;strong&gt;P&lt;/strong&gt; be the set of all paths that go from $s$ to $t$. Recall that we can formulate the max-flow problem as maximizing the sum of flow that goes through each path (assuming each is distinct).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max \sum_{p} f_p&lt;/script&gt;

&lt;p&gt;Constrained by &lt;script type=&quot;math/tex&quot;&gt;\forall e \in E \sum_{p \in P ~s.t.~~ e \in p} f_p \leq c_e&lt;/script&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 18 Oct 2016 00:00:00 -0700</pubDate>
        <link>http://dibya.xyz/beta/2016/10/18/cs170.html</link>
        <guid isPermaLink="true">http://dibya.xyz/beta/2016/10/18/cs170.html</guid>
        
        
      </item>
    
      <item>
        <title>Deskewing MNIST</title>
        <description>&lt;p&gt;When we write, we often write at angles to the paper, which cause letters and numbers to be skewed. Unfortunately, unlike the human eye, computers cannot easily find similarities between images that are transformations of each other. Thus, the process of &lt;em&gt;deskewing&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Very formally, &lt;strong&gt;deskewing&lt;/strong&gt; is the process of straightening an image that has been scanned or written crookedly — that is an image that is slanting too far in one direction, or one that is misaligned.&lt;/p&gt;

&lt;p&gt;In particular, we model the process of deskewing as an affine transformation. We assume that when the image was created (the skewed version), it is actually some affine  skew transformation on the image $ Image’ = A(Image) + b$ which we do not know. What we do know is that we want the center of mass to be the center of the image, and that we’d like to know the angle at which it was skewed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://geometrypreapteacher.wikispaces.com/file/view/skew.jpg/85225087/skew.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The methodology goes as follows:&lt;/p&gt;

&lt;p&gt;1) Find the center of mass of the image to figure out how much we need to offset the image&lt;br /&gt;
2) Find the covariance matrix of the image pixel intensities (we can use this to approximate the skew of the angle)&lt;/p&gt;

&lt;p&gt;The function &lt;strong&gt;moments&lt;/strong&gt; below, calculates these relevant quantities&lt;/p&gt;

&lt;pre&gt; &lt;code class=&quot;python&quot;&gt;
from scipy.ndimage import interpolation

def moments(image):
    c0,c1 = np.mgrid[:image.shape[0],:image.shape[1]] # A trick in numPy to create a mesh grid
    totalImage = np.sum(image) #sum of pixels
    m0 = np.sum(c0*image)/totalImage #mu_x
    m1 = np.sum(c1*image)/totalImage #mu_y
    m00 = np.sum((c0-m0)**2*image)/totalImage #var(x)
    m11 = np.sum((c1-m1)**2*image)/totalImage #var(y)
    m01 = np.sum((c0-m0)*(c1-m1)*image)/totalImage #covariance(x,y)
    mu_vector = np.array([m0,m1]) # Notice that these are \mu_x, \mu_y respectively
    covariance_matrix = np.array([[m00,m01],[m01,m11]]) # Do you see a similarity between the covariance matrix
    return mu_vector, covariance_matrix
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we’d like to calculate the matrix which will allow us to skew “back” to the original image&lt;/p&gt;

&lt;p&gt;This is given by the following formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}1 &amp; 0 \\ \alpha &amp; 1\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\alpha = \frac{Cov(X,Y)}{Var(X)}$&lt;/p&gt;

&lt;p&gt;Furthermore, we have an offset of $\mu - $ center&lt;/p&gt;

&lt;p&gt;Thus, combining the two,  using the handy &lt;em&gt;interpolation&lt;/em&gt; library from scipy, we complete the method &lt;strong&gt;deskew&lt;/strong&gt; below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def deskew(image):
    c,v = moments(image)
    alpha = v[0,1]/v[0,0]
    affine = np.array([[1,0],[alpha,1]])
    ocenter = np.array(image.shape)/2.0
    offset = c-np.dot(affine,ocenter)
    return interpolation.affine_transform(image,affine,offset=offset)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Using L2 Regularized Regression (Ridge Regression), we have&lt;/p&gt;

&lt;h3 id=&quot;unchanged&quot;&gt;Unchanged&lt;/h3&gt;

&lt;p&gt;Train Accuracy: .8564&lt;br /&gt;
Test Accuracy: .8589&lt;/p&gt;

&lt;h3 id=&quot;deskewed&quot;&gt;Deskewed&lt;/h3&gt;

&lt;p&gt;Train Accuracy: .9103&lt;br /&gt;
Test Accuracy: .9140&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thus by using deskewed features, we automatically boost our accuracy rougly 6%! Crazy!&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 03 Oct 2016 00:00:00 -0700</pubDate>
        <link>http://dibya.xyz/beta/2016/10/03/test.html</link>
        <guid isPermaLink="true">http://dibya.xyz/beta/2016/10/03/test.html</guid>
        
        
      </item>
    
      <item>
        <title>Machine Learning Abstractions</title>
        <description>&lt;p&gt;We often write our input &lt;em&gt;data&lt;/em&gt; as $x_1 \dots x_n \in \RR^d$. Our standard for writing &lt;em&gt;labels&lt;/em&gt; $y_1 \dots y_n$ differs dependent on the type of question.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In &lt;em&gt;Unsupervised Learning&lt;/em&gt;: the labels are not given&lt;/li&gt;
  &lt;li&gt;In &lt;em&gt;Classification&lt;/em&gt;, the labels in ${0,1}$&lt;/li&gt;
  &lt;li&gt;In &lt;em&gt;Multi-class classification&lt;/em&gt;, the labels are in ${0,1 \dots k-1}$&lt;/li&gt;
  &lt;li&gt;In &lt;em&gt;Regression&lt;/em&gt;, the labels are in $\RR$
\end{enumerate}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We define a model as a function $F: X \to Y$. It’s corresponding \textit{optimization problem} can be given by 
&lt;script type=&quot;math/tex&quot;&gt;\min_{model} \text{Risk + }\lambda * \textbf{Model Capacity}&lt;/script&gt; where $\lambda$ is the regularization parameter. The algorithms to optimize include&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gradient Descent&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent&lt;/li&gt;
  &lt;li&gt;Singular Value Decomposition &lt;label for=&quot;svd&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;svd&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt; SVD involves much more intense optimization of matrices &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt; Risk &lt;/h2&gt;

&lt;p&gt;Oftentimes when optimizing a model,we aren’t attempting to minimize the largest loss, but rather attempting the minimize the &lt;em&gt;average&lt;/em&gt; loss. Given a loss function $loss(prediction,y)$, we can construct the risk function as follows 
&lt;script type=&quot;math/tex&quot;&gt;R[w] = \Exp[loss] = \int loss(pred,y)p(x,y) ~~dx dy&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;However, the issue is that we don’t know $P(x,y)$, the distribution of the data. The only value that we do know is the sample risk&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{s}[w] = \frac{1}{n}\sum_{1}^n loss(pred_i,y_i)&lt;/script&gt;

&lt;p&gt;We assume that when $n$ is large, that $R_{s}[w] \approx R[w]$.&lt;/p&gt;

&lt;h2&gt; Empirical Risk Minimization &lt;/h2&gt;

&lt;p&gt;\textit{Empirical Risk Minimization} is the process of minimizing the sample risk $R_s[w]$, in the hopes that this translates to a low risk model.&lt;/p&gt;

&lt;p&gt;We define the \textbf{Training Error} $ err_t = \frac{# Incorrect Examples}{# Training Examples}$.&lt;/p&gt;

&lt;p&gt;Further, we define the \textbf{Prediction Error} $err_p$ to be the error rate on unseen data. Notice that if we assume an infinite amount of possible data (from the model), then $err_p = R[w]$&lt;/p&gt;

&lt;p&gt;Finally, we define the \textbf{Generalization Error} to be the difference in the prediction error and the training error.  $err_g = err_p - err_t$.&lt;/p&gt;

&lt;p&gt;With this notation, we can see that&lt;/p&gt;

&lt;p&gt;\begin{center}
	Prediction Error = Generalization Error  + Training Error
	&lt;script type=&quot;math/tex&quot;&gt;R[w] = (R[w] - R_s[w]) + R_s[w]&lt;/script&gt;
\end{center} 
Prediction Error = Generalization Error 
\section{Losses and Regularizations}&lt;/p&gt;

&lt;p&gt;Common Losses for classification&lt;/p&gt;

&lt;p&gt;\begin{enumerate}
	\item Hinge Loss is $L = \max (1 - yw^Tx, 0)$
	\item Least Squares Loss is $L = (1 - yw^Tx)^2 = (y - w^Tx)^2$
	\item Logistic Regression Loss is $L = -yx + \log( exp(-w^Tx) + exp(w^Tx))$
\end{enumerate}&lt;/p&gt;

&lt;p&gt;For \textit{regression}, the loss is often $(y - w^Tx)^2$&lt;/p&gt;

&lt;p&gt;Common Regularizations are the $l_0, l_1, l_2$ metrics.&lt;/p&gt;

&lt;p&gt;\section{Maximum Likelihood Estimation}&lt;/p&gt;

&lt;p&gt;Consider a model of the data which believes that there is some generative model of a given type with parameters $\Theta$ from which we sample $(x,y)$ pairs.&lt;/p&gt;

&lt;p&gt;Our goal is to maximize the probability that the model outputted the values ${(y_i,x_i)}_{i \in N}$. That is, we are trying to maximize
&lt;script type=&quot;math/tex&quot;&gt;\max_\Theta P[\{(y_i,x_i)\} | \Theta]&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In order to simplify calculations, we assume that the samples are independent, giving us&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_\Theta \prod_{i=1}^n P(x_i,y_i | \Theta)&lt;/script&gt;

&lt;p&gt;In order to further simplify (and since probabilities aren’t always concave), we often take the log-likelihood (since most functions are log-convex). Thus, this is equivalent to maximizing
&lt;script type=&quot;math/tex&quot;&gt;\max_\Theta \sum \log P(x_i,y_i | \Theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;\section{Nonlinear Decision Boundaries}&lt;/p&gt;

&lt;p&gt;The easiest way to create nonlinear decision boundaries is to create more featues, thus lifting the data into a higher dimension.&lt;/p&gt;

&lt;p&gt;For example, we can add a bias term by considering the new vector $[ x~~1]^T$ given some vector $x$. We can add features (for example, adding all pairwise interactions between variables). However, this can be very costly, since transforming into a new feature space takes on the order of $O(nd’)$.&lt;/p&gt;
</description>
        <pubDate>Wed, 07 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://dibya.xyz/beta/2016/09/07/cs189.html</link>
        <guid isPermaLink="true">http://dibya.xyz/beta/2016/09/07/cs189.html</guid>
        
        
      </item>
    
      <item>
        <title>Optimization</title>
        <description>&lt;p&gt;Recall from the last &lt;a href=&quot;#&quot;&gt;note&lt;/a&gt; that the loss function that we attempt to minimize for Support Vector Machines is given by 
&lt;script type=&quot;math/tex&quot;&gt;f(w) = \sum_{i=1}^n (1 - y_iw^Tx_i)_+ + \|w\|_2^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;What does it mean to minimize a function?&lt;/p&gt;

&lt;p&gt;We call a point $w’$ a &lt;em&gt;minimizer&lt;/em&gt; (global minimum) if $\forall w~~ f(w^*) \leq f(w)$.&lt;/p&gt;

&lt;p&gt;We call a point $w’$ a &lt;em&gt;local minimizer&lt;/em&gt;(local minimum) if $\exists R&amp;gt; 0$ $\forall w \in B(w’,R)~~f(w’) \leq f(w)$&lt;/p&gt;

&lt;h2&gt; Convexity &lt;/h2&gt;
&lt;p&gt;&lt;label for=&quot;convexpic&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;convexpic&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img src=&quot;http://www.me.utexas.edu/~jensen/ORMM/models/unit/nonlinear/subunits/terminology/graphics/convex1.gif&quot; /&gt; Here’s a picture of a convex function and a secant line  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We call a function $f$ &lt;em&gt;convex&lt;/em&gt; if $\forall w_1, w_2 \in \RR^d ~~ t \in [0,1]$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(tw_1 + (1-t)w_2) \leq tf(w_1) + (1-t)f(w_2)&lt;/script&gt;

&lt;p&gt;This is equivalent to saying that, given any two points, the function falls underneath the line that connects the two end points.&lt;/p&gt;

&lt;p&gt;Here are some ways of checking whether or not a function is convex&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Constant functions $f(x)=c$ are convex&lt;/li&gt;
  &lt;li&gt;Powers of $x$: ($f(x) = x^r$) are convex on the interval $[0,\infty]$ when $r \geq 1$&lt;/li&gt;
  &lt;li&gt;If $f(x)$ is convex, then $f(\mathbf{w^Tx} + \beta)$ is also convex.&lt;/li&gt;
  &lt;li&gt;$-\log(x)$ is convex (Negative Log Likelihood works because of this)&lt;/li&gt;
  &lt;li&gt;If $f(x)$ is concave, then $-f(x)$ is convex.&lt;/li&gt;
  &lt;li&gt;If $f(x)$ and $g(x)$ are convex, then $h(x) = \max {f(x),g(x)}$ is convex.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt; Gradient Descent &lt;/h2&gt;

&lt;p&gt;Gradient descent is a procedure for attempting to find the minimum value of some unbounded function which has a defined gradient at that point. The update equation at each step goes as following:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w_{k+1} = w_{k} - \alpha \nabla f(w_{k})&lt;/script&gt; 
where $\alpha$ is the learning parameter. We claim that $-\nabla f(W_{k})$ is the direction of greatest descent.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Recall that an approximation for $f(w) $ is $f(w_k) + \nabla (w - w_k) + o((w-w_k)^T(w-w_k)) $.&lt;/p&gt;

&lt;p&gt;Considers steps of size $1$, so that $w’ = w + \delta$, where $|\delta|_2^2 = 1$.
We are attempting to maximize $f(w) -f(w’)$ (This is the drop in the function).&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
\arg \max_{\delta} f(w) - f(w') &amp;= \arg \max_{\delta} f(w) - f(w + \delta)\\
 &amp;= \arg \max_{\delta} f(w) - \left(f(w) + \nabla^T((w + \delta) - w)  + o(\|w + \delta - w\|_2^2)\right)\\ 
&amp;= \arg \max_{\delta} -\nabla^Tw - \nabla^T\delta - o(1)\\
\end{align*} %]]&gt;&lt;/script&gt;
Notice that $\nabla^Tw$ is a constant factor, and we further disregard higher-order terms (the $o(1)$)
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;= \arg \max_{\delta}  -\nabla^T\delta \\
\end{align*} %]]&gt;&lt;/script&gt;
Recall from the first homework, that $\arg \max_{|x| = 1}x^Tz = \frac{z}{|z|_1}$, and thus the above expression is maximized when
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
\delta &amp;= \frac{-\nabla}{\|-\nabla\|_1}\\
\delta \propto -\nabla
\end{align*} %]]&gt;&lt;/script&gt;
Thus, the direction of the steepest descent is the opposite of the gradient.&lt;/p&gt;

&lt;p&gt;If $w$ is a local minimum, then we have that $\nabla f(w_0) = 0$. The proof follows from the previous part, since if $w$ is a local minimum, there is no descent direction, and thus $\nabla f$ must be 0.However, the converse is not necessarily true.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; If &lt;em&gt;f&lt;/em&gt; is convex, then $w_*$ is a global minimum if and only if $\nabla f = 0$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; It is clear from the previous part that if $w’$ is a global minimum, then $\nabla f = 0$. We prove then that $\nabla w’ = 0$ implies that $w’$ is a global minimum.&lt;/p&gt;

&lt;p&gt;Recall that since $f$ is convex, we have that $\forall w_1,w_2 ~~ \forall t \in [0,1] ~~ f(t_1w_1 + (1-t_1)w_2) \leq t_1f(w_1) + (1-t_1)f(w_2)$. Consider a point $w^*$ such that $\nabla w’ = 0$. Further, consider an arbitrary other point $w$
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
  f(tw + (1-t)w') &amp;\leq tf(w) + (1-t)f(w')\\
  f(w^* + t(w - w')) - (1-t)f(w')&amp;\leq tf(w)\\
  \frac{f(w^* + t(w - w')) - (1-t)f(w')}{t} &amp;\leq f(w)\\
  f(w^*) + \frac{f(w' + t(w - w')) - f(w)}{t} &amp;\leq f(w)\\
  \end{align*} %]]&gt;&lt;/script&gt;
Let’s consider what happens as $t \to 0$
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
  f(w') + \lim_{t \to 0} \frac{f(w' + t(w - w')) - f(w)}{t}  &amp;\leq f(w)\\
  \end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Notice that this is just a directional derivative
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
  f(w') + \nabla f \dot (w - w') &amp;\leq f(w)\\
  f(w') &amp;\leq f(w)
 \end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2&gt; Psuedocode &lt;/h2&gt;
&lt;pre&gt; &lt;code class=&quot;python&quot;&gt;
def gradientDescent(loss,w,learningRate,steps):
	 for i in range(steps):
		 w = w - learningRate * (loss.gradient(w))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here’s an example of computing the gradient of, and the step equation of the SVM soft-margin classifier.&lt;/p&gt;

&lt;p&gt;First, consider the gradient of $f(x) = max(1 - y_i(w^Tx_i),0)$. This is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\nabla f = \begin{cases}
 0 &amp; y_i(w^Tx_i) &gt; 1\\
 \text{Undefined} &amp; y_i(w^Tx_i) = 1\\
 -y_ix_i &amp; y_i(w^Tx_i) &lt; 1
 \end{cases}) %]]&gt;&lt;/script&gt;

&lt;p&gt;Since the gradient is undefined when $y_i(w^Tx_i) = 1$, we let it be $0$. (Do you notice any connections with the perceptron?).&lt;/p&gt;

&lt;p&gt;Thus, the overall gradient is $c\sum \nabla(f(x_i))  +2w$&lt;/p&gt;

&lt;h2&gt;Stochastic Gradient Descent&lt;/h2&gt;

&lt;p&gt;Gradient descent is a powerful algorithm, but it can potentially be very slow. In particular, a single iteration of gradient descent takes time $O(n)$ where $n$ is the number of training points that we have. In the case that $n$ is very large, these iterations are quite costly. Thus, we introduce a new idea.&lt;/p&gt;

&lt;p&gt;Instead of computing the gradient on the entire training set, we compute the gradient of the loss on a random data point picked at each iteration, and travel down, this way.&lt;/p&gt;

&lt;p&gt;Assuming that we select a random point $i: (x_i,y_i)$, notice that the gradient of loss is a noisy version of the true gradient. Thus,
&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}(\nabla loss_i(w)) = \nabla loss(w)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Notice that just because $\Exp(\nabla loss_i(w^&lt;em&gt;)) = 0$, it doesn’t mean that $\forall i~~loss_i(w^&lt;/em&gt;) = 0$. Similarly, just because the current loss that we created has a zero gradient, it doesn’t mean that we have a local minimum.&lt;/p&gt;

&lt;h3&gt; Why Randomness? &lt;/h3&gt;

&lt;p&gt;Why do we use random points, instead of points in the order of the dataset? It could be that the data is arranged in a bad (perhaps even adverserial way) such that the time till convergence is significantly increased. If we select random particles, the probability that we get such a bad sequence is reduced to a low number.&lt;/p&gt;

&lt;h3&gt; Region Of Confusion &lt;/h3&gt;

&lt;p&gt;Let us assume that we can draw the individual loss functions for each data point. They (looking at it in 2 dimensions for simplicity) form a picture like this \~&lt;br /&gt;
\marginnote{As per Dimitri Bertsekas}
\includegraphics*[]{regionofconfusion}
What this shows us is that when the weight is at the end, the stochastic gradients tend to align and agree, and so we have fast and uniform movement in the “farout” regions. However, once in the center, the individual data points begin to disagree, causing the region of confusion. This eventually leads to bouncing around, causing a slowdown, and prevention of SGD. We can prevent this using the tips below.&lt;/p&gt;

&lt;h2&gt; Tips for SGD&lt;/h2&gt;

&lt;h3&gt; Epochs &lt;/h3&gt;

&lt;p&gt;Instead of choosing an element at random every time, we can shuffle the data once, and run through this shuffled order (which is essentially random without replacement), and repeat once finished. This speeds up the procedure inside the epoch.&lt;/p&gt;

&lt;h3&gt;Decaying Learning Rate&lt;/h3&gt;

&lt;p&gt;As a general rule of thumb, you should pick step sizes as large as possible without diverging. You can further anneal the learning rate every epoch($N = N/2$ every epoch), or use a form of exponential decay ($N(t) = (1- \lambda)N(t-1) \approx N_0 e^{-\lambda t}$).&lt;/p&gt;

&lt;h3&gt;Momentum&lt;/h3&gt;

&lt;p&gt;Essentially emulating a ball rolling down the gradient hill, we pretend as though the ball has some momentum (and thus velocity), and this allows us to guide towards the optimum faster. If the previous direction was good, then it also continues down the direction. (By default, start with momentum parameter $\beta = 0.9$)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;v_{i} = \beta v_{i-1} - \alpha \nabla(g)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;w_{i} = w_{i-1} + v_i&lt;/script&gt;&lt;/p&gt;

&lt;h2&gt;Newton's Method&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;This section is from Lecture 12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Newton’s method is another way of finding optima of our loss function, which requires many less iterations in exchange for expensive computations.&lt;/p&gt;

&lt;p&gt;How the method works is by approximating a quadratic polynomial about the point you’re currently at, and jump to the minimum of the quadratic (where the derivative of the quadratic is $0$).&lt;/p&gt;

&lt;p&gt;Recall the second order taylor series of $f$ about some point $w$:&lt;label for=&quot;Matrix Derivatives&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;Matrix Derivatives&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;The matrix derivatives that we use here are 
$\frac{\partial f(x)^Tg(x)}{x} = \frac{\partial f(x)}{x}g(x) + \frac{\partial g(x)}{x}f(x)$
$ \frac{\partial g(x)^TAf(x)}{x} = \frac{\partial g}{x}Af(x) + \frac{\partial f}{x}A^Tg(x)$
 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
f(w') &amp;= f(w) + \nabla^T(w'-w) + \frac{1}{2}(w'-w)^T\nabla^2 (w'-w)\\
\end{align*} %]]&gt;&lt;/script&gt;
We’d like to find the optimum over all $w’$  and so we take the derivative of this expression w.r.t. $w$ &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{\partial f(w')}{\partial w'} &amp;= -\nabla + \frac{1}{2}\left((-I)(\nabla^2(w'-w)) + (-I)(\nabla^{2T}(w'-w)\right)\\
\text{Recall that the Hessian is symmetric}&amp;\\
\frac{\partial f(w')}{\partial w'} &amp;= -\nabla + \frac{1}{2}\left(-(\nabla^2(w'-w))-(\nabla^{2}(w'-w)\right)\\
\frac{\partial f(w')}{\partial w'} &amp;= -\nabla -(\nabla^2(w'-w))\\
\end{align*} %]]&gt;&lt;/script&gt;
Setting this to $0$, and solving for $w’$
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
0 &amp;= \nabla + \nabla^2(w'-w)\\
\nabla^2w - \nabla &amp;= \nabla^2w'\\
(\nabla^2)^{-1}(\nabla^2w - \nabla) &amp;= w'\\
w' &amp;= w - (\nabla^2)^{-1} \nabla
\end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Although this is a nice closed-form solution for the next update step, due to numerical instability with solutions of the inverse of $\nabla^2$, we often solve the step right before it.\sidenote{For example, with scipy.linalg.solve} 
&lt;script type=&quot;math/tex&quot;&gt;w' = (\nabla^2)^{-1}(\nabla^2w - \nabla)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Newton’s method can also be analogized to the version you learned in single-variable calculus, as trying to find the &lt;em&gt;zeros&lt;/em&gt; of the derivative, in order to find optima.&lt;/p&gt;

&lt;p&gt;Newton’s method often converges much faster (in the number of iterations), since it exploits more information about the local surface than gradient descent does. In fact, due to the construction of the problem, it will always converge in exactly $1$ step on quadratics, and have superior performance on loss functions that are similar to quadratic.&lt;/p&gt;

&lt;p&gt;However, Newton’s method comes at a high computational cost. We have to compute the Hessian of the function (which has $d^2$ entries) on each iteration (since our function changes). In practice, Newton’s method is not used, simply because of cost of computing the Hessian Matrix. However, alternative methods such as BFGS, which approximate portions of the Hessian, often work &lt;em&gt;almost as well&lt;/em&gt;, and are often used in the field.&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://dibya.xyz/beta/2016/09/01/cs189.html</link>
        <guid isPermaLink="true">http://dibya.xyz/beta/2016/09/01/cs189.html</guid>
        
        
      </item>
    
      <item>
        <title>Linear Classifiers</title>
        <description>&lt;p&gt;What does machine learning aim to do?&lt;/p&gt;

&lt;p&gt;Given &lt;em&gt;data&lt;/em&gt; $x_1 \dots x_n$ and &lt;em&gt;properties&lt;/em&gt; $y_1 \dots y_n$, we attempt to predict $y(x)$ for some $x$ that is not in the sample &lt;em&gt;data&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Abstractly, we build functions of the type&lt;/p&gt;
&lt;pre&gt; &lt;code class=&quot;python&quot;&gt;
def predict(x):
	y = miracle(x)
	return y
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In this class we make the following assumption&lt;/p&gt;

&lt;p&gt;$x_i$ live in $\mathbb{R}^d$: we can write each data point $x_i$ as a feature vector, where each entry is a feature. When we consider the data matrix $X$ (with dimensions $n \times d$), we let $X_{ij}$ be the $j$-th feature of the $i$-th example.&lt;/p&gt;

&lt;h2&gt; Linear Classifiers &lt;/h2&gt;

&lt;p&gt;We consider the following classifier for binary classification (where $Y \in {0,1}$).&lt;/p&gt;

&lt;p&gt;We essentially define a scoring function defined by 
&lt;script type=&quot;math/tex&quot;&gt;f(x) = w^Tx + \beta&lt;/script&gt; and then classifying $y = 1$ if $f(x) \geq 0$ and $y = 0$ if $f(x) &amp;lt; 0$ \marginnote{This is equivalent $y_i = \mathbb{1}(\beta^T x_i \geq \tau)$ for some threshold $\tau = -\beta$}&lt;/p&gt;

&lt;pre&gt; &lt;code class=&quot;python&quot;&gt;
def predict(x):
	v = np.dot(w,x)
	if v &amp;gt; threshold:
		return True
	return False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The decision boundary for this linear classifier is a hyperplane which divides the space into two halves. If a new datapoint is one on side of the decision boundary, we classify it as $1$, and otherwise as $0$. The decision boundary is given by 
&lt;script type=&quot;math/tex&quot;&gt;\{x \in \RR^d: w^Tx + \beta= 0\}&lt;/script&gt;&lt;/p&gt;

&lt;h2&gt; Properties of the Boundary &lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1:&lt;/strong&gt; &lt;label for=&quot;boundaryproof&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;boundaryproof&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For proof, consider $x,y$ on the plane, so we have that $x-y$ is on the plane. Since $x$ and $y$ are on the plane, we know that $w^Tx + beta = 0$ and $w^Ty + beta =0$, and subtracting the equations that $w^T(x-y) = 0$, or that $w bot (x-y)$ &lt;/span&gt; We have that $w$ is the normal vector of this hyperplane (that it is orthogonal to all vectors in the plane)&lt;/p&gt;

&lt;p&gt;This theorem allows us to notice that the smallest vector that goes from an arbitrary point $x$ to the boundary will be a scalar multiple of $w$. That is, if $\tau$ is the shortest distance to the boundary, then we have that 
$x + \tau \frac{w}{|w|} \in \text{Boundary}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt;
&lt;label for=&quot;marginproof&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;marginproof&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;For proof, notice that we are simply solving for $tau$ when $x = \vec{0}$. We have that $w^T(\tau \frac{w}{|w|}) + \beta = 0$. Solving for $tau$, we have $\tau \frac{|w|_2^2}{|w|} = -beta$, or $\tau = \frac{-\beta}{|w|}$ &lt;/span&gt;
The distance from the origin of $\RR^d$ to the decision boundary is given by $- \frac{\beta}{|w|}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; The (signed) distance from a vector to the boundary is given by $\frac{f(x)}{|w|}$&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; For an arbitrary vector $x$, the distance to the boundary can be found by solving the original equation for the shortest vector.&lt;/p&gt;

&lt;div class=&quot;mathblock&quot;&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{align*}
w^T(x + \tau \frac{w}{\|w\|}) + \beta &amp;= 0\\
\tau \frac{\|w\|^2}{\|w\|}  &amp;= -\beta - w^Tx\\
\tau \|w\| &amp;= -f(x)\\
\tau &amp;= -\frac{f(x)}{\|w\|}
\end{align*}
&lt;/script&gt;&lt;/div&gt;

&lt;p&gt;We’d like to use these results to analyze how good a classifier is. For this, we shift notation to let $y_i = {-1,1}$, which means that if we multiply $y_if(x_i)$, then if the result is positive, then we have classified correctly; otherwise we have that the classification is incorrect.&lt;/p&gt;

&lt;p&gt;We define the &lt;em&gt;margin&lt;/em&gt; of a classifier as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{margin} = \min_{i} \frac{y_i f(x_i)}{\|w\|}&lt;/script&gt;

&lt;p&gt;Our goal is to now find a classifier that maximizes the margin. To simplify, first assume that the data is linearly separable. This is equivalent to saying that the margin is positive (nonzero and not negative).  Since $\exists w ~~\forall i ~~ (y_i f(x_i)) &amp;gt; 0$, letting $\alpha = \min_{i}{y_if(x_i)}$, we know that when $w’ = \frac{1}{\alpha} w$, then $\forall i ~~ (y f(x_i)) &amp;gt; 1$.&lt;/p&gt;

&lt;p&gt;We can now use this fact to now try to maximize the margin: &lt;script type=&quot;math/tex&quot;&gt;w' = \max_{w}  \frac{y_if(x_i)}{\|w\|}&lt;/script&gt; subject to $y_if(x_i) &amp;gt; 0$. However, we can rewrite this as &lt;script type=&quot;math/tex&quot;&gt;w' = \min_{w} \|w\|&lt;/script&gt; subject to $y_if(x_i) &amp;gt; 1$&lt;/p&gt;

&lt;h2&gt; Perceptrons &lt;/h2&gt;

&lt;p&gt;The previous maximization problem required quadratic programming to solve, and this may not be the best approach. Instead, we introduce perceptrons. In essence, we attempt to train a linear classifier, updating the weights as we go to try to match the data better.&lt;/p&gt;

&lt;pre&gt; &lt;code class=&quot;python&quot;&gt;
for data point (x,y) in data (continue this stream forever):
	if (y*f(x) &amp;lt; 0):
		w = w + y*x
		b = b+ y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt;&lt;label for=&quot;perceptronproof&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;perceptronproof&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Proof at &lt;a href=&quot;http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf&quot;&gt;http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf&lt;/a&gt; &lt;/span&gt; If the data is linearly separable, then no matter the order of updates, the perceptron algorithm will terminate with a correct solution. The algorithm will converge in at most $\frac{R^2}{\gamma^2}$, where $R = \max |x_i|$ and $\gamma$ is the margin.&lt;/p&gt;

&lt;h2&gt; Slack &lt;/h2&gt;

&lt;p&gt;Let’s assume that the data is not linearly separable; what do we do in order to find a good margin fit. We introduce a concept called &lt;em&gt;slack&lt;/em&gt;. We relax our constraints to $\forall i ~~ y_iw^Tx_i \geq 1 - \epsilon_i$ where $\epsilon_i &amp;gt; 0$. However, we must reduce incentive to use slack, and we add it to the minimization term; Thus our new minimization becomes&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\min_{w} \|w\|_2^2 + c \sum \epsilon_i&lt;/script&gt; such that $\forall i ~~ y_iw^Tx_i \geq 1 - \epsilon_i$.&lt;/p&gt;

&lt;p&gt;Notice that this is equivalent to minimizing 
&lt;script type=&quot;math/tex&quot;&gt;\min_{w} c \sum_{i=1}^n (1 - y_iw^Tx_i)_+ + \|w\|_2^2&lt;/script&gt;
where $(x)_+ = max(0,x)$&lt;/p&gt;

&lt;p&gt;Here’s what happens when you play around with the parameter $c$:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;small C&lt;/th&gt;
      &lt;th&gt;large C&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Desire&lt;/td&gt;
      &lt;td&gt;maximize margin $\frac{1}{|w|}$&lt;/td&gt;
      &lt;td&gt;Keep most slack variables small (or zero)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Danger&lt;/td&gt;
      &lt;td&gt;Underfitting (misclassifies much training data)&lt;/td&gt;
      &lt;td&gt;Overfitting&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Outliers&lt;/td&gt;
      &lt;td&gt;less sensitive&lt;/td&gt;
      &lt;td&gt;very sensitive&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Wed, 31 Aug 2016 00:00:00 -0700</pubDate>
        <link>http://dibya.xyz/beta/2016/08/31/cs189.html</link>
        <guid isPermaLink="true">http://dibya.xyz/beta/2016/08/31/cs189.html</guid>
        
        
      </item>
    
      <item>
        <title>Interpolating Polynomials</title>
        <description>&lt;p&gt;I’ve been reading up on polynomials and their properties lately, and a certain question caught my eye .&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Adam and Betty are playing a game related to guessing polynomials. Adam has thought up a polynomial (no limit on degrees) with all positive integer coefficients, which we call P(x). Betty can ask for the values of the polynomial at any integer x, and using only this information, she must discern what Adam’s polynomial is. What is the maximum number of queries that Betty will have to make?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Your first inclination might be that the number of queries depends on the degree of the polynomial, but I’ll make the claim that Betty will only require 2 guesses to guess Adam’s polynomial. The key step that allows us to discern an nth degree polynomial in constant queries is a process called adaptive querying. By using the result of the first query, we can determine a value x, which will allow us to extract the values of the coefficients, no matter the degree of  the polynomial.&lt;/p&gt;
&lt;h6 id=&quot;the-game-of-adaptive-queries&quot;&gt;The Game of Adaptive Queries&lt;/h6&gt;

&lt;p&gt;Let’s find a strategy so that it will only take 2 queries to solve the problem. We can represent Adam’s polynomial as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = a\_0 + a\_1x + a\_2x^2 + \dots + a\_nx^n&lt;/script&gt;

&lt;p&gt;where $a_0, a_1, a_2 \dots a_n$  represent the coefficients of the numbers. Notice the similarity between the formula for this polynomial, and the representation of a number in base_x. For instance, a number given as $a_1a_0$ in base 10 can be rewritten as $a_0 + 10*a_1$. Similarly, a number written as $a_2a_1a_0$ in base 5 can be rewritten as $a_0 + a_1(5)^2 + a_2(5)^3$.Since the coefficients of Adam’s coefficients are all positive, his polynomial is analogous as rewriting a number from base $x$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a\_0 + a\_1x + a\_2x^2 + \dots + a\_{n-1}x^{n-1} + a\_nx^n = a\_na\_{n-1} \dots a\_2a\_1a\_0 ~~(base x)&lt;/script&gt;

&lt;p&gt;We must be careful though, since this property will only hold when x is larger than all of the coefficients (otherwise we might have carrying over, which distorts the data). How do we pick $x$ then? We now must find a way to find the value of the highest possible coefficient. Notice that $P(1)$ is simply $a_0 + a_1 + a_2 + \dots + a_n$, which is always greater than or equal to the largest coefficient (since all coefficients are positive). Taking x to be $P(1)+1$ guarantees that we now have a base that will properly encode the values of the coefficients. The final step remains to convert this number to base $x+1$.&lt;/p&gt;

&lt;h6 id=&quot;time-to-code&quot;&gt;Time to Code&lt;/h6&gt;

&lt;p&gt;To summarize the previous paragraph, our strategy is to first query for $P(1)$, then $P(~P(1) +1~) $&lt;/p&gt;

&lt;p&gt;I’ll be coding this example in Python: the choice of language is arbitrary, so feel free to use whatever language suits your fancy. Some structural code: the function &lt;em&gt;make_polynomial&lt;/em&gt; creates a polynomial f(x) whose coefficients are that of the list that you passed in, and the function &lt;em&gt;tobase&lt;/em&gt; converts a number into certain base (outputs a list of digits).&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_polynomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coefficients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Given a list of coefficients: [a0, a1, a2, ... an], returns a polynomial
		function which computes the value of the polynomial at any given x&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coefficients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tobase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Converts a num to base x&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;coefficients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;coeff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;coefficients&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coeff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coefficients&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Our strategy is now quite simple:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;guess_coefficients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Given a polynomial function p, finds its coefficients &quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tobase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;taking-it-further&quot;&gt;Taking it Further&lt;/h4&gt;

&lt;p&gt;Now let’s look at a variant of this problem, except a tad harder:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Adam has now thought of a polynomial (of nth degree), whose coefficients are real numbers. Betty can ask for the values of the polynomial at any integer x, and using only this information, she must discern what Adam’s polynomial is. What is the maximum number of queries that Betty will have to make?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now the coefficients are no longer positive integers, so our base-changing strategy will no longer work. So let’s start afresh: simple intuition leads us to believe that it should take n+1 points to determine this new polynomial. After all, a 0 degree polynomial ($y=a_0$) is uniquely identified by one point, and a 1 degree polynomial (a line $y = a_0 + a_1x$) by 2 points. Let’s try to put mathematics behind this intuition:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem: A polynomial of degree $n$ is uniquely identified by $n+1$ points&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $P_n$ represent the set of all polynomials with real coefficients up to degree n&lt;/p&gt;

&lt;p&gt;Let T be a linear transformation given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(P) = \begin{bmatrix} P(0) \cr P(1) \cr P(2) \cr \vdots \cr P(N) \end{bmatrix}&lt;/script&gt;

&lt;p&gt;Our goal is to prove that the linear transformation $T(p)$ is an injection from the set of polynomials to $R^{n+1}$. In other words, any polynomial can be represented by a unique vector in $R^{n+1}$&lt;/p&gt;

&lt;p&gt;First let us observe the kernel of this transformation. Remember that the kernel of a tranformation is a subset of the domain which maps to the zero element of the codomain. The only polynomial in $P_n$ that maps to the zero vector is $0$, since $T(0) =  \begin{bmatrix} P(0) \cr P(1) \cr P(2) \cr \vdots \cr P(N) \end{bmatrix} $ leads to the zero vector, $\begin{bmatrix} 0 \cr 0 \cr 0 \cr \vdots \cr 0 \end{bmatrix} $. Since the polynomial $ p(x) = 0$ is the only element that maps to zero, we can consider the kernel (null space) to be trivial. By definition, since the kernel is trivial, the transformation is injective, and our theorem is proven.&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Nov 2015 00:00:00 -0800</pubDate>
        <link>http://dibya.xyz/beta/2015/11/15/polynomials.html</link>
        <guid isPermaLink="true">http://dibya.xyz/beta/2015/11/15/polynomials.html</guid>
        
        
      </item>
    
  </channel>
</rss>
