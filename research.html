---
layout: layout
title: "Research Overview"
---

<h1 class="page-heading">Research</h1>
<center>  Here's my <a href="https://scholar.google.com/citations?user=znnl0kwAAAAJ&hl=en&oi=sra">Google Scholar</a>. My <a onclick='alert("Dibya Ghosh -> Ani Adhikari -> Persi Diaconis -> Paul Erdos")'>Erd≈ës number</a> is 3. My Bacon number is lamentably still undefined.  </center>

<div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:3em" src="/images/research/vice.png">
    </div>
    
    <div class="nine columns">
      
      <b><a href="https://sites.google.com/view/inverse-event"> Variational Inverse Control with Events</a></b>
      <p> Justin Fu*, Avi Singh*, <i> Dibya Ghosh</i>, Larry Yang, Sergey Levine <br/>
      NIPS 2018
    </p>
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/1805.11686">ArXiv</a></td>
          <td><a href="https://sites.google.com/view/inverse-event">Website</a></td>
          <td></td>
        </tr>
      </table>
      <div class="four columns"></div>
      <div class="four columns"></div>
      <div class="three columns"></div>
  <br />   
   <p>In this paper, we present VICE, a method which generalizes inverse optimal control to learning reward functions from goal examples. By requiring only goal examples, and not demonstration
     trajectories, VICE is well suited for real-world robotics tasks, in which reward specification is difficult.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

  
<div class="row">
  
  
  <div class="three columns">
<img style="margin-top:3em" src="/images/research/dnc.png">
  </div>
  
  <div class="nine columns">
    
    <b><a href="http://dibyaghosh.com/dnc/">Divide-and-Conquer Reinforcement Learning</a></b>
    <p><i> Dibya Ghosh</i>, Avi Singh, Aravind Rajeswaran, Vikash Kumar, Sergey Levine <br/>
    ICLR 2018.
  </p>
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1711.09874">ArXiv</a></td>
        <td><a href="http://dibyaghosh.com/dnc/">Website</a></td>
        <td><a href="https://github.com/dibyaghosh/dnc">Code</a></td>
      </tr>
    </table>
    <div class="four columns"></div>
    <div class="four columns"></div>
    <div class="three columns"></div>
<br />   
 <p>In this paper, we present a method for scaling model-free deep reinforcement learning methods to tasks with high stochasticity
      in initial state and task distributions. We demonstrate our method on a suite of challenging sparse-reward manipulation tasks that were unsolved by prior work.
    </p>
  </div>


  </div>
  
  
</div>
<div class="divider"></div>

