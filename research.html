---
layout: layout
title: "Research Overview"
---

<h1 class="page-heading">Research</h1>
<center>  Here's my <a href="https://scholar.google.com/citations?user=znnl0kwAAAAJ&hl=en&oi=sra">Google Scholar</a>. My <a onclick='alert("Dibya Ghosh -> Ani Adhikari -> Persi Diaconis -> Paul Erdos")'>Erd≈ës number</a> is 3. My Bacon number is lamentably still undefined.  </center>

<div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:3em" src="/images/research/arc.png">
    </div>
    
    <div class="nine columns">
      
      <b><a href="https://arxiv.org/abs/1811.07819"> Learning Actionable Representations with Goal-Conditioned Policies</a></b>
      <p> <i> Dibya Ghosh</i>, Abhishek Gupta, Sergey Levine <br/>
      NeurIPS Deep RL Workshop 2018
    </p>
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/1811.07819">ArXiv</a></td>
          <td><a href="https://sites.google.com/view/arc-reps">Website</a></td>
          <td></td>
        </tr>
      </table>
      <div class="four columns"></div>
      <div class="four columns"></div>
      <div class="three columns"></div>
  <br />   
   <p>In this paper, we present ARCs, a representation learning algorithm which attempts to optimize <i> functionally relevant </i> elements of state. ARCs relate distance between states in latent space with the actions required to reach the state, which implicitly captures system dynamics and ignores uncontrollable factors. ARCs are useful for exploration, features for policies, and for developing hierarchies.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:3em" src="/images/research/vice.png">
    </div>
    
    <div class="nine columns">
      
      <b><a href="https://sites.google.com/view/inverse-event"> Variational Inverse Control with Events</a></b>
      <p> Justin Fu*, Avi Singh*, <i> Dibya Ghosh</i>, Larry Yang, Sergey Levine <br/>
      NeurIPS 2018
    </p>
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/1805.11686">ArXiv</a></td>
          <td><a href="https://sites.google.com/view/inverse-event">Website</a></td>
          <td></td>
        </tr>
      </table>
      <div class="four columns"></div>
      <div class="four columns"></div>
      <div class="three columns"></div>
  <br />   
   <p>In this paper, we present VICE, a method which generalizes inverse optimal control to learning reward functions from goal examples. By requiring only goal examples, and not demonstration
     trajectories, VICE is well suited for real-world robotics tasks, in which reward specification is difficult.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

  
<div class="row">
  
  
  <div class="three columns">
<img style="margin-top:3em" src="/images/research/dnc.png">
  </div>
  
  <div class="nine columns">
    
    <b><a href="http://dibyaghosh.com/dnc/">Divide-and-Conquer Reinforcement Learning</a></b>
    <p><i> Dibya Ghosh</i>, Avi Singh, Aravind Rajeswaran, Vikash Kumar, Sergey Levine <br/>
    ICLR 2018.
  </p>
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1711.09874">ArXiv</a></td>
        <td><a href="http://dibyaghosh.com/dnc/">Website</a></td>
        <td><a href="https://github.com/dibyaghosh/dnc">Code</a></td>
      </tr>
    </table>
    <div class="four columns"></div>
    <div class="four columns"></div>
    <div class="three columns"></div>
<br />   
 <p>In this paper, we present a method for scaling model-free deep reinforcement learning methods to tasks with high stochasticity
      in initial state and task distributions. We demonstrate our method on a suite of challenging sparse-reward manipulation tasks that were unsolved by prior work.
    </p>
  </div>


  </div>
  
  
</div>
<div class="divider"></div>

