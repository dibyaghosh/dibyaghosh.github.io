---
layout: layout
title: "Research Overview"
---

<h1 class="page-heading">Research</h1>
<center> My <a onclick='alert("Dibya Ghosh -> Ani Adhikari -> Persi Diaconis -> Paul Erdos")'>Erd≈ës number</a> is 3. My Bacon number is lamentably still undefined. See my <a href="https://scholar.google.com/citations?user=znnl0kwAAAAJ&hl=en&oi=sra">Google Scholar</a> for more.</center>


<div class="divider"></div>
<!-- <h2 id='preprints' class="page-heading">Publications</h1> -->
  <div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:0em" src="/images/research/pgoperators.png">
  <table>
    <tr>
      <td><a href="https://arxiv.org/abs/2006.11266">ArXiv</a></td>
      <td><a href="https://www.youtube.com/watch?v=Ol9GwgBalhA">Talk</a></td>
        <td><a href="http://mcmachado.info/?p=248">Blog</a></td>
    </tr>
  </table>    
</div>
    
    <div class="nine columns">
      
      <b><a href=""> An operator view of policy gradient methods </a></b>
      <p> <i>Dibya Ghosh</i>, Marlos C. Machado, Nicolas Le Roux<br/>
        NeurIPS 2020
      </p>
   <p> We reinterpret the classical <span style='font-variant-caps: all-small-caps;'>REINFORCE</span> algorithm as the iterated application of two operators:
    a policy improvement operator and a projection operator. This new perspective provides new insights about
    the behavior of policy gradient methods, and in particular, uncovers a strong link with their value-based counterparts.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

  <div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:0em" src="/images/research/stablerepr.png">
  <table>
    <tr>
      <td><a href="https://arxiv.org/abs/2007.05520">ArXiv</a></td>
      <td><a href="https://icml.cc/virtual/2020/poster/6666">Talk</a></td>
        <!-- <td><a href="https://github.com/google-research/google-research/tree/master/memento">Code</a></td> -->
    </tr>
  </table>    
</div>
    
    <div class="nine columns">
      
      <b><a href=""> Representations for Stable Off-Policy Reinforcement Learning
      </a></b>
      <p><i>Dibya Ghosh</i>, Marc G. Bellemare<br/>
      ICML 2020
    </p>
   <p>
     We study the learning dynamics of the classic off-policy value function learning algorithm, <span style='font-variant-caps: all-small-caps;'>TD(0)</span>, through the lens of the state representation.
      Our work reveals several insights into how certain choices of state representation affect stability and divergence of RL.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>


  <div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:0em" src="/images/research/memento.jpg">
  <table>
    <tr>
      <td><a href="https://arxiv.org/abs/2002.12499">ArXiv</a></td>
      <td><a href="https://slideslive.com/38921889/biological-and-artificial-reinforcement-learning-4">Talk</a></td>
        <td><a href="https://github.com/google-research/google-research/tree/master/memento">Code</a></td>
    </tr>
  </table>    
</div>
    
    <div class="nine columns">
      
      <b><a href=""> On Catastrophic Interference in Atari 2600 Games</a></b>
      <p> Liam Fedus*, <i>Dibya Ghosh*</i>, John Martin, Yoshua Bengio, Marc G. Bellemare, Hugo Larochelle<br/>
        NeurIPS Biological and Artificial RL Workshop 2019 <span style='color:red'>(Oral)</span>
      </p>
  
   <p> We present the MEMENTO observation: that training a fresh agent which starts from the state that a trained agent plateaus can greatly improve performance.
     We show this manifests across a number of learning algorithms across all 60 games in the Atari Learning Environment. We demonstrate how this simple observation can induce a
     simple end-to-end agent which better avoids learning plateaus.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:0em" src="/images/research/gcsl.png">
  <table>
    <tr>
      <td><a href="https://arxiv.org/abs/1912.06088">ArXiv</a></td>
      <td><a href="https://dibyaghosh.com/blog/rl/gcsl.html">Blog</a></td>
      <td><a href="https://github.com/notdibya/gcsl">Code</a></td>
    </tr>
  </table>    
</div>
    
    <div class="nine columns">
      
      <b><a href="https://arxiv.org/abs/1912.06088"> Learning To Reach Goals Without Reinforcement Learning </a></b>
      <p> <i> Dibya Ghosh*</i>, Abhishek Gupta*,  Justin Fu, Ashwin Reddy, Coline Devin, Benjamin Eysenbach, Sergey Levine <br/>
      Preprint.
    </p>
   <p> We present an algorithm (GCSL) which learns goal-reaching behaviors through iterative supervised learning. Drawing insights from imitation learning and hindsight
     experience replay, we show that a specific relabelling scheme for goals and actions induces a "dataset" of expert behavior. By using supervised learning, the algorithm
     avoids the optimization challenges present in value function methods and policy gradient schemes.
      </p>
    </div>
  
  
    </div>
    <div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:0em" src="/images/research/arc.png">
  <table>
    <tr>
      <td><a href="https://arxiv.org/abs/1811.07819">ArXiv</a></td>
      <td><a href="https://sites.google.com/view/arc-reps">Website</a></td>
    </tr>
  </table>    
</div>
    
    <div class="nine columns">
      
      <b><a href="https://arxiv.org/abs/1811.07819"> Learning Actionable Representations with Goal-Conditioned Policies</a></b>
      <p> <i> Dibya Ghosh</i>, Abhishek Gupta, Sergey Levine <br/>
      ICLR 2019
    </p>
   <p> We present a representation learning algorithm (ARC) which attempts to optimize for <i> functionally relevant </i> elements of state. ARC relates distance between states in latent space with the actions required to reach the state, implicitly capturing system dynamics and ignoring uncontrollable factors. ARC is useful for exploration, features for policies, and for developing hierarchies.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

<div class="row">
  
  
    <div class="three columns">
  <img style="margin-top:0em" src="/images/research/vice.png">
  <table>
    <tr>
      <td><a href="https://arxiv.org/abs/1805.11686">ArXiv</a></td>
      <td><a href="https://sites.google.com/view/inverse-event">Website</a></td>
    </tr>
  </table>
      
</div>
    
    <div class="nine columns">
      
      <b><a href="https://sites.google.com/view/inverse-event"> Variational Inverse Control with Events</a></b>
      <p> Justin Fu*, Avi Singh*, <i> Dibya Ghosh</i>, Larry Yang, Sergey Levine <br/>
      NeurIPS 2018
    </p>
      
   <p>We present VICE, a method which generalizes inverse optimal control to learning reward functions from goal examples. By requiring only goal examples, and not demonstration
     trajectories, VICE is well suited for real-world robotics tasks, in which reward specification is difficult.
      </p>
    </div>
  
  
    </div>
    
      <div class="divider"></div>

  
<div class="row">
  
  
  <div class="three columns">
<img style="margin-top:0em" src="/images/research/dnc.png">
<table>
  <tr>
    <td><a href="https://arxiv.org/abs/1711.09874">ArXiv</a></td>
    <td><a href="http://dibyaghosh.com/dnc/">Website</a></td>
    <td><a href="https://github.com/dibyaghosh/dnc">Code</a></td>
  </tr>
</table>
  </div>
  
  <div class="nine columns">
    
    <b><a href="http://dibyaghosh.com/dnc/">Divide-and-Conquer Reinforcement Learning</a></b>
    <p><i> Dibya Ghosh</i>, Avi Singh, Aravind Rajeswaran, Vikash Kumar, Sergey Levine <br/>
    ICLR 2018.
  </p>
 <p>We present a method for scaling model-free deep reinforcement learning methods to tasks with high stochasticity
      in initial state and task distributions. We demonstrate our method on a suite of challenging sparse-reward manipulation tasks that were unsolved by prior work.
    </p>
  </div>


  </div>
  
  
</div>
<div class="divider"></div>

